{
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<span style=\"font-size:200%\">MCMC</span><br>\n<span style=\"color: gray\">dic 2019</span><br>\n[*Alberto Ruiz*](http://dis.um.es/profesores/alberto)",
      "metadata": {
        "lang": "es"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Las técnicas **Markov-Chain-Monte-Carlo** permiten obtener muestras de una densidad de probabilidad cuando solo es posible evaluar una función proporcional a dicha densidad (normalmente porque es intratable normalizarla). Esto es suficiente para resolver el problema de la inferencia Bayesiana mediante simulación.",
      "metadata": {
        "lang": "es"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Introducción",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "En muchas situaciones los datos observables $D$ dependen de los parámetros $\\theta$ de un modelo siguiendo una ley conocida:\n\n$$p(D \\,|\\, \\theta )$$\n\nEl problema fundamental de la inferencia probabilística es obtener información acerca los parámetros $\\theta$ cuando se han observado unos datos concretos $D_o$. Es decir, buscamos la probabilidad condicionada contraria:\n\n$$p( \\theta \\,|\\, D_{o} )$$\n\nLos parámetros $\\theta$ son inicialmente desconocidos, aunque normalmente disponemos de cierta información $p(\\theta)$, \"a priori\", sobre ellos.\n\nPor tanto, estamos interesados en\n\n$$p( \\theta \\,|\\, D_{o} ) = \\frac{p(D_o,\\theta)}{p(D_o)} = \\frac{p(D_o,\\theta)}{\\sum_\\theta p(D_o,\\theta)} \\propto p(D_o,\\theta)$$\n\nEs decir, la distribución *a posteriori* es proporcional a la conjunta, evaluada en los datos observados, y donde quedan como variables los parámetros del modelo. La distribución conjunta es el producto del modelo de medida y la información a priori:\n\n$$p(D_o,\\theta) = p(D_o\\mid\\theta)\\; p(\\theta)$$\n\nPor tanto\n\n$$p( \\theta \\mid D_{o} ) \\propto p( D_o \\mid \\theta ) \\; p(\\theta)$$\n\n\n\n\nLa información sobre los parámetros se expresa a su vez mediante hiperparámetros sobre los que se tiene de nuevo cierta información, y así sucesivamente.\n\n$$P(D,\\theta, \\alpha) =  P(D\\mid\\theta,\\alpha)\\;p(\\theta,\\alpha) = P(D\\mid\\theta)\\; p(\\theta \\mid \\alpha)\\; p(\\alpha) $$\n\nEn definitiva, la densidad conjunta se expresa como un producto de densidades condicionadas explotando las dependencias del modelo.\n\n\n\nCuando el número de variables es pequeño se pueden aplicar técnicas de *grid*, recorriendo exhaustivamente el espacio de parámetros. (De hecho, los ejemplos de este notebook se pueden resolver fácilmente de esta forma.)\n\nPero en problemas interesantes esta cadena de factorizaciones puede ser bastante compleja, con una constante de normalización computacionalmente intratable. En algunas aplicaciones puede bastar con encontrar el parámetro más probable. Pero normalmente interesa también la incertidumbre de la estimación, lo que implica analizar regiones más amplias del espacio de parámetros.\n\n\nLas técnicas MCMC permiten muestrear eficientemente cualquier densidad sin necesidad de que esté normalizada. Podemos aplicarlo a factorizaciones del tipo $P(D_o\\mid\\theta)\\;p(\\theta \\mid \\alpha)\\; p(\\alpha)$, y utilizar las muestras para extraer información sobre $p(\\theta \\mid D_o)$, marginalizando los parámetros auxiliares (\"nuisance\") que no nos interesen.\n\n$$P(D,\\theta) =  \\sum_\\alpha\\; P(D,\\theta,\\alpha)$$\n\nEsto se consigue de forma inmediata ignorando los valores de $\\alpha$ en la muestra obtenida.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Estas [transparencias de Lam](http://pareto.uab.es/mcreel/IDEA2017/Bayesian/MCMC/mcmc.pdf) lo explican con más detalle.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Implementación del algoritmo *Metropolis*",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Para trabajar en serio es recomendable utilizar paquetes profesionales como [stan](http://mc-stan.org/) o [pymc3](http://docs.pymc.io/).\n\nSin embargo, para familizarizarnos con la técnica podemos experimentar con una implementación sencilla de la variante más simple: el algoritmo de [Metropolis](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm).",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "El objetivo es obtener muestras de $p(x)$ pero solo podemos evaluar $f(x)=k\\; p(x)$, donde $k$ se desconoce. Dada la muestra actual $x_{k}$ generamos la muestra siguiente $x_{k+1}$ de acuerdo con la siguiente receta: generamos una perturbación $x_p$ alrededor de $x_k$ y calculamos el ratio de verosimilitudes $\\rho =\\min( f(x_p) / f(x_k), 1)$. Aceptamos $x_p$ como muestra siguiente con probabilidad $\\rho$, y si la rechazamos repetimos la muestra anterior.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Código",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# paso elemental\ndef metropolisGen(sigma, x0, lprob):\n    s = (np.array(x0), lprob(x0), True)\n    while True:\n        yield s\n        xa, la, _ = s\n        delta = sigma * np.random.randn(len(xa))\n        x = xa + delta\n        l = lprob(x)\n        ratio = l - la\n        accept = ratio > 0 or np.log(np.random.rand()) < ratio\n        if accept:\n            s = (x,l,True)\n        else:\n            s = (xa,la,False)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from itertools import islice\nfrom sys import stderr\n\n# generación de muestras\ndef metropolis(lprob, n, burn, step, sigma, x0):\n    run    = metropolisGen(sigma,x0,lprob)\n    select = islice(run , burn , burn+n*step , step)\n    sample, accept = zip(*[(s,a) for s,_,a in select])\n    print('ρ = {:.3f}'.format(np.mean(accept)), file=stderr)\n    return np.array(sample)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "A continuación comprobamos su funcionamiento en un par de ejemplos sencillos.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Muestreo de una normal 2D",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "La forma razonable de hacerlo es el método directo:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "mu  = np.array([1,1])\ncov = np.array([[1,   0.8],\n                [0.8, 1  ]])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "x,y = np.random.multivariate_normal(mu,cov,50).T",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n%matplotlib inline",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.plot(x,y,'.'); plt.axis('equal');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Vamos a comprobar si el algoritmo de Metropolis consigue un resultado similar.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def lgauss(m, c):\n    ic = np.linalg.inv(c)\n    return lambda x: -0.5* (x-m) @ ic @ (x-m)     # + k",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test1 = metropolis(lgauss(mu,cov), n=100, burn=0, step=1, sigma=1, x0=[-3,3])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(6,6))\nx,y = test1.T\nplt.plot(x,y,'.-',markersize=8,lw=0.5); plt.axis('equal');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test2 = metropolis(lgauss(mu,cov), n=1000, burn=100, step=5, sigma=1, x0=[-3,3])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(6,6))\nx,y = test2.T\nplt.plot(x,y,'.',markersize=8, alpha=0.3); plt.axis('equal');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test2.mean(axis=0)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "np.cov(test2.T)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Muestreo de una densidad toroidal",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "lg = lgauss([2],[[0.3**2]])\n\ntest3 = metropolis(lambda v: lg(np.linalg.norm(v)), n=300, burn=1000, step=5, sigma=1, x0=[-3,3],)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(6,6))\nx,y = test3.T\nplt.plot(x,y,'.',markersize=8); plt.axis('equal');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Justificación intuitiva",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Consideremos el algoritmo de Metropolis en el caso más simple posible: una distribución discreta con solo dos elementos, $a$ y $b$, con $P(a) = 2P(b)$. Las transiciones son de $b\\rightarrow a$ siempre, y $a \\rightarrow b$ o $a \\rightarrow a$ al 50%. Por tanto, en términos de \"media muestra\", $b$ genera dos mitades de $a$ y $a$ genera media $a$ y media $b$. Para la proporción final da igual usar medias muestras que muestras completas.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "from collections import Counter\n\ndef samp(x):\n    return x.replace('a','AB').replace('b','AA').lower()\n\ns = 'b'\nfor k in range(10):\n    s = samp(s)\n    c = Counter(s)\n    print(s)\n    print(c, c['b']/c['a'])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Estas secuencias son los posibles estados finales después de 10 transiciones, todos ellos equiprobables. Si el sistema es ergódico el promedio temporal iguala al espacial, y por tanto la historia seguida es equivalente a los estados finales posibles.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Cuando las reglas de transformación se aplican con las proporciones que tiene cada elemento producen esas mismas proporciones: $1b,2a \\rightarrow 2a, 2a,2b = 4a,2b$. Es la distribución estacionaria a la que tiende la cadena de Markov.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Ejemplos de aplicación",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Media y varianza",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Deseamos estimar la media y la dispersión de una variable normal con prioris no informativas, a partir de una pequeña muestra.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "data = np.random.randn(10)/2 + 1\n\nprint(np.mean(data),np.std(data))\n\ndata",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "En primer lugar suponemos conocida la dispersión.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def lgauss1d(m, s, x):\n    return -0.5 * ((x-m)/s)**2 - np.log(s)\n\ndef logprob(D):\n    def f(θ):\n        [m] = θ\n        s = 0.5\n        return sum(lgauss1d(m,s,D)) + 0\n    return f\n\nprint(logprob(data)([0.8]))\nprint(logprob(data)([1]))",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Ajustamos `sigma` para conseguir un $\\rho$ alrededor de 0.3-0.4.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "test = metropolis(logprob(data), n=5000, burn=1000, step=3, sigma=0.5, x0=[0] )\nh = plt.hist(test, bins=np.linspace(-3,3,40), color='lightgreen', edgecolor=\"gray\")\nplt.xlim(-3,3); plt.xlabel('media');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "En segundo lugar suponemos conocida la media.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def ljeffreys(s):\n    return -np.log(s)\n\ndef logprob(D):\n    def f(θ):\n        [s] = θ\n        if s <= 0: return -1e10\n        m = 1\n        return sum(lgauss1d(m,s,D)) + ljeffreys(s)\n    return f\n\ntest = metropolis(logprob(data),n=5000, burn=1000, step=3, sigma=0.3, x0=[1])\nh = plt.hist(test, bins=20, color='lightgreen', edgecolor=\"gray\")\nplt.xlim(0,3); plt.xlabel('desviación');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Y finalmente suponemos que tanto media como desviación son desconocidas.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def logprob(D):\n    def f(θ):\n        m,s = θ\n        if s <= 0: return -1e10\n        return sum(lgauss1d(m,s,D)) + ljeffreys(s)\n    return f\n\ntest = metropolis(logprob(data), n=5000, burn=1000, step=3, sigma=0.2, x0=[0,1] )\nm,s = test.T\nplt.figure(figsize=(6,6))\nplt.plot(m,s,'.',markersize=2, alpha=0.5);\nplt.xlabel('med'); plt.ylabel('std');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nh = plt.hist(test[:,0], bins=20, color='lightgreen', edgecolor=\"gray\")\nplt.xlim(-3,3); plt.xlabel('mean')\nplt.subplot(1,2,2)\nh = plt.hist(test[:,1], bins=20, color='lightgreen', edgecolor=\"gray\");\nplt.xlabel('sigma');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Algunas de estas distribuciones a posteriori se pueden obtener de forma analítica. (Por ejemplo, la distribución de la media muestral cuando se desconoce la desviación es una *t-student*.) Pero en casos más generales solo podemos recurrir a técnicas computacionales. ",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Outliers",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Veamos lo que ocurre si los datos están contaminados con *outliers*.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "noisydata = np.append(data,[-4,2,4])\nnoisydata",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test = metropolis(logprob(noisydata), n=5000, burn=1000, step=3, sigma=0.7, x0=[0,1] )\nm,s = test.T\nplt.figure(figsize=(6,6))\nplt.plot(m,s,'.',markersize=2, alpha=0.5);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "%pip install -q https://raw.githubusercontent.com/albertoruiz/jupyterlite/main/content/misc/umucv-0.3-py3-none-any.whl\n\nimport umucv.prob as pr\n\ndef toProb(histo):\n    h,b,_ = histo\n    x = (b[:-1] + b[1:])/2\n    return pr.P({x:h for x,h in zip(x,h)})\n\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nh = plt.hist(m, bins=20, color='lightgreen', edgecolor=\"gray\")\npr.showhdi(toProb(h),95)\nplt.xlim(-3,3); plt.xlabel('mean')\nplt.subplot(1,2,2)\nh = plt.hist(s, bins=20, color='lightgreen', edgecolor=\"gray\")\npr.showhdi(toProb(h),95)\nplt.xlim(0,3); plt.xlabel('sigma');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Lógicamente se obtienen estimaciones mucho más dispersas. Es necesario incorporar al modelo la posibilidad de que existan outliers, lo cual se puede hacer mediante un modelo de mezcla.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def lunif(a,b,p):\n    return -np.log(b-a) if a<=p<=b else -1e10\n\ndef gaussian1d(m,s,x):\n    return 1/np.sqrt(2*np.pi)/s * np.exp ( -0.5 * ((x-m)/s)**2 )\n\ndef rmod(p,m,s,x):\n    return np.log( (1-p)*gaussian1d(m ,s, x) + p* gaussian1d(0, 5, x) )\n\nrmod(0.2,0,1,noisydata)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def logprob(D):\n    def f(θ):\n        p,m,s = θ\n        if s <= 0: return -1e10\n        if not (0 <= p <= 1): return -1e8\n        return sum(rmod(p,m,s,D)) + ljeffreys(s) + 0 + lunif(0,1,p)\n    return f\n\nlogprob(noisydata)([0.2,0,1])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "logprob(noisydata)([0.3,1,0.5])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test = metropolis(logprob(noisydata), n=5000, burn=1000, step=3, sigma=0.15, x0=[0.5,0,1])\np,m,s = test.T\nplt.figure(figsize=(6,6))\nplt.plot(m,s,'.',markersize=2, alpha=0.5);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "h = plt.hist(m,bins=30, color='lightblue');\npr.showhdi(toProb(h),95)\nplt.xlim(-3,3);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "h = plt.hist(s,bins=30, color='lightblue');\npr.showhdi(toProb(h),95);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "h = plt.hist(p,bins=np.linspace(0,1,30), color='lightblue');\npr.showhdi(toProb(h),95)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Las estimaciones son ahora casi tan precisas como con datos limpios, y además tenemos una estimación sobre la proporción de outiliers en la muestra.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Lo más interesante es que si realizamos el mismo proceso con datos limpios, el método lo detecta dando una alta probabilidad a proporciones pequeñas de outiliers. La inferencia bayesiana controla automáticamente la capacidad del modelo.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "test = metropolis(logprob(data), n=5000, burn=1000, step=3, sigma=0.1, x0=[0.5,0,1])\np,m,s = test.T\nplt.figure(figsize=(6,6))\nplt.plot(m,s,'.',markersize=2, alpha=0.5);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "h = plt.hist(p,bins=np.linspace(0,1,20), color='orange');\npr.showhdi(toProb(h),95)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Encuesta",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "¿Qué se puede decir de las proporciones reales de una cierta característica (por ejemplo, el porcentaje de votantes a unos partidos), a partir de una pequeña muestra?",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Antes de empezar comprobamos que la forma de expresar las proporciones es correcta, sin favorecer a ninguna de ellas. Hay tres proporciones $p,q,r$ pero solo 2 grados de libertad ya que $r=1-p-q$.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def logprob(p):\n    p,q = p\n    if p+q>1: return -1e10\n    return lunif(0,1,p) + lunif(0,1,q)\n\ntest5 = metropolis(logprob, 5000, 1000, 3, 0.35, [0.2, 0.2])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "x,y = test5.T\nplt.figure(figsize=(6,6))\nplt.plot(x,y,'.',markersize=1); plt.axis('equal'); plt.axis([0,1,0,1]);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "np.mean(x), np.mean(y), np.mean(1-x-y)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.hist(x,alpha=0.3,bins=20); plt.hist(y,alpha=0.3,bins=20); plt.hist(1-x-y, alpha=0.3,bins=20);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Ahora definimos el modelo de medida. Utilizamos el distribución multinomial, que nos dice la probabilidad de obtener $n_1,n_2,n_3$... éxitos de cada categoría cuando sus probabilidades son $p_1,p_2,p_3$... Es una generalización de la binomial.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def lfact(n):\n    return sum(np.log(np.arange(1,n+1)))\n\ndef lmultinom(ns, ps):\n    return lfact(sum(ns)) - sum([lfact(n) for n in ns]) + sum([n*np.log(p) for n,p in zip(ns,ps)])\n\nprint(lmultinom([2,3,5],[0.2,0.4,0.4]))\nprint(lmultinom([2,3,5],[0.2,0.3,0.5]))\nprint(lmultinom([0,10,2],[0.2,0.3,0.5]))",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "En un primer experimento tenemos muy pocas respuestas, por lo que el resultado es bastante incierto.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def logprob(D):\n    def f(θ):\n        p,q = θ\n        if p+q>1 or min(p,q)<0: return -1e10\n        return lmultinom(D, [p,q,1-p-q]) + lunif(0,1,p) + lunif(0,1,q)\n    return f\n\ntest5 = metropolis(logprob([5,3,2]), 10000, 2000, 3, 0.2, [0.2, 0.2])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "x,y = test5.T\nplt.figure(figsize=(6,6))\nplt.plot(x,y,'.',markersize=2, alpha=0.2); plt.axis('equal'); \nplt.plot([0,1],[1,0],ls='dashed',color='gray',lw=0.5); plt.grid(ls='dotted');\nplt.axis([0,1,0,1]);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "La mismas proporciones en una muestra más amplia reducen la incertidumbre.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "test = metropolis(logprob([50,30,20]), 10000, 2000, 3, 0.06, [0.2, 0.2])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "x,y = test.T\nplt.figure(figsize=(6,6))\nplt.plot(x,y,'.',markersize=2, alpha=0.2); plt.axis('equal'); \nplt.plot([0,1],[1,0],ls='dashed',color='gray',lw=0.5); plt.grid(ls='dotted');\nplt.axis([0,1,0,1]);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Ahora podemos evaluar la probabilidad de sucesos concretos:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "np.mean(x>0.45)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "np.mean(x>0.5)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "np.mean( y > (1-x-y) )",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Podemos obtener estimaciones puntuales de los parámetros junto con su incertidumbre, pero no son independientes. Las estimaciones están centradas en el dato observado con 4-5% de desviación. Pero lógicamente no todos pueden estar a la vez en el extremo superior de su intervalo.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "np.mean(x), np.mean(y), np.mean(1-x-y)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "np.std(x), np.std(y), np.std(1-x-y)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Billar raro",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "El problema que aparece en el [blog de Jake VanderPlas](http://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/).",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def logprob(p):\n    [p] = p\n    if not (0<=p<=1): return -1e10\n    return lunif(0,1,p) + 3*np.log(p) + 5*np.log(1-p)\n\ntest4 = metropolis(logprob, 1000, 500, 10, 0.5, [0.5])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "np.mean(test4**3)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "h = plt.hist(test4,bins=np.linspace(0,1,20), color='lightgreen', edgecolor=\"gray\")\npr.showhdi(toProb(h),95)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from itertools import repeat\n\ndef bernoulli(p,a,b):\n    return pr.P({a:p, b:1-p},norm=False)\n\nseguir = (lambda b1: pr.joint(repeat(bernoulli(b1,0,1) ,3))) & toProb(h)\nlist(seguir.items())[:5]",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "seguir.marginal(lambda s: sum(s[:3]))",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "seguir.prob(lambda s: sum(s[:3])==0)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}