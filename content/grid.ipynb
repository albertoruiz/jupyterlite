{
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<span style=\"font-size:200%\">Técnicas de *grid*</span><br>\n<span style=\"color: gray\">dic 2019</span><br>\n[*Alberto Ruiz*](http://dis.um.es/profesores/alberto)",
      "metadata": {
        "lang": "es"
      }
    },
    {
      "cell_type": "markdown",
      "source": "La inferencia Bayesiana puede resolverse mediante exploración exhaustiva cuando el número de parámetros es pequeño.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Normal con outliers",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Normal con outliers",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom matplotlib import rc\n#rc('text', usetex=True)\n#rc('font', size=14)\n\nG = np.random.randn\n\ndef show1ddata(x, sz=(8,0.5), **kwargs):\n    plt.figure(figsize=sz)\n    \n    options = { 'marker': 'x', 's': 60, 'alpha': 0.75, 'color':'blue' }\n    options.update(kwargs)\n    \n    plt.ylim(-1,1);\n    plt.scatter(x,x*0, zorder=5, **options);\n    \n    ax = plt.gca()\n    ax.spines['left'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_yticklabels([])\n    ax.set_yticks([])\n    ax.spines['bottom'].set_position('zero')\n    ax.spines['bottom'].set_color('gray')\n    \n    col = 'gray'\n    ax.tick_params(axis='x', colors=col)\n    \ndef shbracket(x, k=2):\n    m = x.mean()\n    s = x.std()\n    d = 0.6\n    plt.fill_between([m-k*s,m+k*s], [d,d], -d, alpha=0.2, color='green')\n    \ndef shrange(x1,x2,d):\n    plt.fill_between([x1,x2], [d,d], -d, alpha=0.2, color='green')",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Partimos de una muestra de una variable aleatoria gaussiana.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "n = 10\n\nμ = 1\nσ = 0.5\n\ndata = μ  +  σ * G(n)\n#print(data)\nshow1ddata(data, color='blue', alpha=0.5)\nplt.xlim(-5,5);",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Lo usual es calcular la media y la desviación estándar. Cuanto mayor sea $n$ más se aproximarán al valor real.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "print(np.mean(data), np.std(data))\nshow1ddata(data, color='blue', alpha=0.5)\nplt.xlim(-5,5);\nshbracket(data)",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "A veces los datos disponibles están contaminados con  \"outliers\", y lo que es peor, es posible que la proporción  $\\varepsilon$ de dichos outliers sea desconocida.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "noisydata = np.append(data,[-4.3, 2.2, 4.1])\n\nshow1ddata(noisydata, color='blue', alpha=0.5)\nplt.xlim(-5,5);\nshbracket(noisydata)",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from numpy import log\n\ndef normalize(x):\n    return x/x.sum()\n\ndef lgauss1d(m, s, x):\n    return -0.5 * ((x-m)/s)**2 - log(s)\n\n\ndef ljeffreys(s):\n    return -log(s) if s > 0 else MINF\n\n\ndef logprob(D):\n    def f(θ):\n        m,s = θ\n        return sum(lgauss1d(m,s,D)) + ljeffreys(s)\n    return f",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Discretizamos los parámetros.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "n = 100\nM = np.linspace(0,2,n)\nS = np.linspace(0.1,1,n+1)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Evaluamos la verosimilitud $\\times$ prior de todas las configuraciones, y al normalizar tenemos las probabilidades a posteriori.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "f = logprob(data)\n\nP = normalize(np.exp([[f((m,s)) for m in M] for s in S]))",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Mostramos gráficamente la densidad conjunta:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "from mpl_toolkits.mplot3d import Axes3D\n\nm,s = np.meshgrid(M,S)\n\nfig = plt.figure(figsize=(12,4))\nax = fig.add_subplot(121, projection='3d')\nax.plot_surface(m,s,P, cmap='coolwarm', linewidth=0.5, rstride=2, cstride=2);\nax.set_zticks([]);\nax.set_xticks(np.linspace(0,2,5))\nax.set_title('$P(\\mu \\sigma| D)$');\nax.set_xlabel('$\\mu$'); ax.set_ylabel('$\\sigma$');\n\nplt.subplot(1,2,2)\nplt.imshow(-np.flipud(P),'gray');\nax = plt.gca()\nax.set_xticks([0,len(M)-1]); ax.set_xticklabels(M[[0,-1]]);\nplt.xlabel('$\\mu$')\nax.set_yticks([0,len(S)-1]); ax.set_yticklabels(S[[-1,0]]);\nplt.ylabel('$\\sigma$');",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def levels(P, probs=[0.99, 0.9, 0.5]):\n    vals = np.sort(P.flatten())[::-1]\n    cum  = np.cumsum(vals)\n    v = vals[[np.where(cum > p)[0][0] for p in probs ]]\n    fmt = {v:f'{100*p:.0f}%' for v,p in zip(v,probs)}\n    return v,fmt",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Es más informativo mostrar las curvas de nivel que engloban diferentes cantidades de probabilidad acumulada:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(5,5))\nlev,fmt = levels(P, probs=[0.99,0.95, 0.9,0.8, 0.5])\n\nCS = plt.contour(M,S,P,colors='black',levels=lev);\nplt.grid(ls='dashed');plt.xlabel('$\\mu$'); plt.ylabel('$\\sigma$');\nplt.clabel(CS,CS.levels,fmt=fmt,fontsize=10);\nplt.title('$\\mathcal{P}(\\,\\mu,\\sigma\\;|\\;D\\,)$',fontsize=16);",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Las dos densidades marginales son:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,4))\n\nplt.subplot(1,2,1);\nplt.plot(M, P.sum(axis=0)); plt.xlabel('$\\mu$'); plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\mu\\;|\\;D\\,)$')\n\nplt.subplot(1,2,2);\nplt.plot(S, P.sum(axis=1)); plt.xlabel('$\\sigma$'); plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\sigma\\;|\\;D\\,)$');",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Ampliamos el modelo con la proporción de outliers. Se trata de una mezcla de la gaussiana \"real\" con otra muy ancha que recoge los outliers.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def gaussian1d(m,s,x):\n    return 1/np.sqrt(2*np.pi)/s * np.exp ( -0.5 * ((x-m)/s)**2 )\n\ndef rmod(e,m,s,x):\n    return log( (1-e)*gaussian1d(m ,s, x) + e* gaussian1d(0, 5, x) )",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Calculamos el bloque de probabilidades a posteriori, ahora 3D. ",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "n = 100\nM = np.linspace(0,2,n)\nS = np.linspace(0.1,1,n+1)\nE = np.linspace(0.01,0.99,n+2)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Requiere bastante más tiempo de cómputo al hacerlo con listas de Python.\n# Aprox 1m vs menos de 2s usando la vectorización de np\n# ¿Cuánto tarda con GPU?\n\nif False:\n    \n    def logprob(D):\n        def f(θ):\n            e,m,s = θ\n            return sum(rmod(e,m,s,D)) + ljeffreys(s) # + 0 + lunif(0,1,p)\n        return f\n\n    f = logprob(data)\n\n    P = normalize(np.exp([[[f((e,m,s)) for m in M] for s in S] for e in E]))\n\n    plt.figure(figsize=(5,5))\n    plt.contour(M,S,P.sum(axis=0),colors='black'); plt.grid(ls='dashed');plt.xlabel('$\\mu$'); plt.ylabel('$\\sigma$');\n    plt.title('$\\mathcal{P}\\,(\\,\\mu,\\sigma\\;|\\;D\\,)$',fontsize=16);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# aprovechamos la vectorización de numpy\n\ne,s,m = np.meshgrid(E,S,M,indexing='ij')\n\n# En vez de esta suma explícita se puede crear un bloque con todos los datos, \n# expandiendo adecuadamente las dimensiones, y efectuando la suma con np, pero\n# me ha resultado menos eficiente\n\nL = sum([rmod(e,m,s,d) for d in data]) - log(s)\nP = normalize(np.exp(L))\n\nPms = P.sum(axis=0)\n\nplt.figure(figsize=(5,5))\n\nlev,fmt = levels(Pms, probs=[0.99, 0.9, 0.5])\nCS = plt.contour(M,S,Pms,colors='black',levels=lev);\nplt.clabel(CS,CS.levels,fmt=fmt,fontsize=10);\n    \nplt.grid(ls='dashed');plt.xlabel('$\\mu$'); plt.ylabel('$\\sigma$');\nplt.title('$\\mathcal{P}(\\,\\mu,\\sigma\\;|\\;D\\,)$',fontsize=16);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "Pes = P.sum(axis=2)\n\nplt.figure(figsize=(5,5))\n\nlev,fmt = levels(Pes, probs=[0.99, 0.9, 0.5])\nCS = plt.contour(S,E,Pes,colors='black',levels=lev);\nplt.clabel(CS,CS.levels,fmt=fmt,fontsize=10);\n    \nplt.grid(ls='dashed');plt.xlabel('$\\sigma$'); plt.ylabel('$\\epsilon$');\nplt.title('$\\mathcal{P}(\\,\\epsilon,\\sigma\\;|\\;D\\,)$',fontsize=16);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(15,4))\n\nplt.subplot(1,3,1);\nplt.plot(M, P.sum(axis=(0,1))); plt.xlabel('$\\mu$'); plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\mu\\;|\\;D\\,)$')\n\nplt.subplot(1,3,2);\nplt.plot(S, P.sum(axis=(0,2))); plt.xlabel('$\\sigma$'); plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\sigma\\;|\\;D\\,)$');\n\nplt.subplot(1,3,3)\nplt.plot(E, P.sum(axis=(1,2))); plt.xlabel('$\\epsilon$');  plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\epsilon\\;|\\;D\\,)$');",
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Repetimos el proceso, pero ahora con los datos contaminados. Aprovechamos para medir el tiempo de cálculo.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "L = sum([rmod(e,m,s,d) for d in noisydata]) - log(s)\nPn = normalize(np.exp(L))\n\nPms = Pn.sum(axis=0)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(5,5))\n\nlev,fmt = levels(Pms, probs=[0.99, 0.9, 0.5])\nCS = plt.contour(M,S,Pms,colors='black',levels=lev);\nplt.clabel(CS,CS.levels,fmt=fmt,fontsize=10);\n    \nplt.grid(ls='dashed');plt.xlabel('$\\mu$'); plt.ylabel('$\\sigma$');\nplt.title('$\\mathcal{P}(\\,\\mu,\\sigma\\;|\\;D\\,)$',fontsize=16);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "Pes = Pn.sum(axis=2)\n\nplt.figure(figsize=(5,5))\n\nlev,fmt = levels(Pes, probs=[0.99, 0.9, 0.5])\nCS = plt.contour(S,E,Pes,colors='black',levels=lev);\nplt.clabel(CS,CS.levels,fmt=fmt,fontsize=10);\n    \nplt.grid(ls='dashed');plt.xlabel('$\\sigma$'); plt.ylabel('$\\epsilon$');\nplt.title('$\\mathcal{P}(\\,\\epsilon,\\sigma\\;|\\;D\\,)$',fontsize=16);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(15,4))\n\nplt.subplot(1,3,1);\nplt.plot(M, Pn.sum(axis=(0,1))); plt.xlabel('$\\mu$'); plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\mu\\;|\\;D\\,)$')\n\nplt.subplot(1,3,2);\nplt.plot(S, Pn.sum(axis=(0,2))); plt.xlabel('$\\sigma$'); plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\sigma\\;|\\;D\\,)$');\n\nplt.subplot(1,3,3)\nplt.plot(E, Pn.sum(axis=(1,2))); plt.xlabel('$\\epsilon$');  plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\epsilon\\;|\\;D\\,)$');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(15,4))\n\nplt.subplot(1,3,1);\nplt.plot(M, P.sum(axis=(0,1)));\nplt.plot(M, Pn.sum(axis=(0,1))); plt.xlabel('$\\mu$'); plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\mu\\;|\\;D\\,)$')\n\nplt.subplot(1,3,2);\nplt.plot(S, P.sum(axis=(0,2)));\nplt.plot(S, Pn.sum(axis=(0,2))); plt.xlabel('$\\sigma$'); plt.yticks([]); plt.title('$\\mathcal{P}(\\,\\sigma\\;|\\;D\\,)$');\n\nplt.subplot(1,3,3)\nplt.plot(E, P.sum(axis=(1,2)));\nplt.plot(E, Pn.sum(axis=(1,2))); plt.xlabel('$\\epsilon$');  plt.yticks([]);  plt.title('$\\mathcal{P}(\\,\\epsilon\\;|\\;D\\,)$');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Los estimadores de $\\mu$ y $\\sigma$ son parecidos con y sin outliers, pero la existencia de outliers y su proporción son correctamente detectadas.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Intervalo de confianza",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Primero comprobamos el método tradicional para construir un [intervalo de confianza](https://en.wikipedia.org/wiki/Confidence_interval) para $\\mu$. Cuando $\\sigma$ [se desconoce](https://en.wikipedia.org/wiki/Normal_distribution#Confidence_intervals) necesitamos la $t$ de Student.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "# comprobaciones iniciales\n\nfrom scipy.stats import t\nfrom scipy.stats import norm\n\nts = t(5-1)\n\ng = norm()\n\ndeltat = ts.ppf(0.5+.95/2)\ndeltat = ts.ppf(1-0.05/2)\nprint(deltat)\n\ndeltan = g.ppf(0.5+.95/2)\nprint(deltan)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def exper(n, method='true'):\n    mu1,mu2 =   -3,3\n    sig1,sig2 = 0.2, 1\n    mu =  np.random.rand()*(mu2-mu1) + mu1\n    sig = np.random.rand()*(sig2-sig1) + sig1\n    sample1 = np.random.randn(n)*sig + mu\n\n    emu  = sample1.mean()\n    esig = sample1.std()*n/(n-1)\n\n    deltan = g.ppf(0.5+.95/2)\n    deltat = t(n-1).ppf(0.5+.95/2)\n    \n    # comprobamos que la mu y sigma estimadas no engloban la proporción\n    # hay que sacarlo a una comprobación desde fuera\n    if method == 'check':\n        sample2 = np.random.randn(10000)*sig + mu\n        prop  = ((sample2 < mu + deltan*sig) & (sample2 > mu - deltan*sig)).mean()\n        eprop = ((sample2 < emu + deltan*esig) & (sample2 > emu - deltan*esig)).mean()\n        return prop, eprop\n    \n    if method=='true':\n        # IC con el verdadero sigma\n        ic1 = emu - deltan*sig/np.sqrt(n)\n        ic2 = emu + deltan*sig/np.sqrt(n)        \n        return ic1 < mu < ic2\n    \n    if method=='gaussian':\n        # IC mal calculado con el sigma observado\n        ic1 = emu - deltan*esig/np.sqrt(n)\n        ic2 = emu + deltan*esig/np.sqrt(n)\n        return ic1 < mu < ic2\n    \n    if method=='student':\n        # IC bien calculado con la t de student\n        ic1 = emu - deltat*esig/np.sqrt(n)\n        ic2 = emu + deltat*esig/np.sqrt(n)\n        return ic1 < mu < ic2\n    \n    raise NameError",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "El 95% de las muestras están en $\\mu \\pm 1.96 \\sigma$, pero no entre $\\bar \\mu \\pm 1.96 s$.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "exper(5, 'check')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "El intervalo de confianza para $\\mu$ se debe construir con la verdadera $\\sigma$ y una gaussiana o con su estimación $s$ y la t de Student.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "np.mean([ exper(5,'true') for _ in range(10000) ])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "np.mean([ exper(5, 'gaussian') for _ in range(10000) ])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "np.mean([ exper(5, 'student') for _ in range(10000) ])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "La alternativa es aplicar Bayes directamente marginalizando $\\sigma$.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "n = 100\nM = np.linspace(-3,3,n)\nS = np.linspace(0.1,3,n+1)\n#S = np.linspace(0.49,0.51,n+1)\ns,m = np.meshgrid(S,M,indexing='ij')\n\ndef BayesInterval(D, show=False):\n    L = sum([lgauss1d(m,s,d) for d in D]) # + ljeffreys(s)\n    Pms = normalize(np.exp(L))\n    Pm = Pms.sum(axis=0)\n    Ps = Pms.sum(axis=1)\n    m1,m2 = M[np.where(Pm > levels(Pm,[0.95])[0][0])[0][[0,-1]]]\n    if show:\n        plt.figure(figsize=(8,4))\n        plt.subplot(1,2,1); plt.plot(M,Pm);\n        shrange(m1,m2,d=Pm.max()/50)\n        plt.subplot(1,2,2); plt.plot(S,Ps);\n    return m1,m2\n\nN = 5\nmu = 1\nsig = 0.5\ndat = mu + sig*G(N)\nBayesInterval(dat,show=True);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Cuando repetimos el experimento la media queda dentro el porcentaje de veces deseado. El procedimiento Bayesiano directo consigue \"automáticamente\" el efecto de usar la t de Student.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "N = 5\ntot = 0\ntrials = 1000\nfor k in range(trials):\n    dat = mu + sig*G(N)\n    x1,x2 = BayesInterval(dat,show=False)\n    tot += x1 < mu < x2\nprint(f'{tot/trials*100:.0f}%')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Pendiente: usar el generador con el prior utilizado.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Predictive distributions",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Y otra cuestión interesante es la de conseguir la distribución de nuevas muestras. Podemos construir una gaussiana con los estimadores más verosímiles $\\bar x$ y $s$, o podemos marginalizar su distribución conjunta dados los datos. Esto va a ensanchar la distribución final, teniendo en cuenta automáticamente el tamaño de la muestra.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def nice():\n    ax = plt.gca()\n    ax.spines['left'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_yticklabels([])\n    ax.set_yticks([])\n    ax.spines['bottom'].set_position('zero')\n    ax.spines['bottom'].set_color('gray')\n    \n    col = 'gray'\n    ax.tick_params(axis='x', colors=col)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# trabaja con M,S,X,m,s globales\n# FIXME: fijo a 95%\ndef BayesPredictInterval(D, method='good', show=False, truef=None):\n    \n    n = len(D)\n    \n    if n>1:\n        emu  = D.mean()\n        esig = D.std()*n/(n-1)\n    \n    if method=='bad':\n        return emu-1.96*esig , emu+1.96*esig\n\n    L = sum([lgauss1d(m,s,d) for d in D])\n\n    x = X.reshape(-1, *np.ones(L.ndim,int))\n\n    L = lgauss1d(m,s,x) + L\n    \n    P = normalize(np.exp(L))\n    Px = P.sum(axis=(1,2))\n    m1,m2 = X[np.where(Px > levels(Px,[0.95])[0][0])[0][[0,-1]]]\n    \n    if show:\n        Pm = P.sum(axis=(0,1))\n        Ps = P.sum(axis=(0,2))\n        plt.figure(figsize=(12,4))\n        plt.subplot(1,3,1); plt.plot(M,Pm); plt.xlabel('$\\mu$'); plt.yticks([])\n        plt.subplot(1,3,2); plt.plot(S,Ps); plt.xlabel('$\\sigma$'); plt.yticks([])\n        plt.subplot(1,3,3); plt.plot(X,Px,lw=3,label='Bayes')\n        if truef is not None:\n            plt.plot(X, normalize(truef.pdf(X)),label='true')\n            plt.xlabel('$x$'); plt.yticks([])\n        options = { 'marker': 'x', 's': 60, 'alpha': 0.50, 'color':'blue' }\n        plt.scatter(D,n*[0], zorder=5, **options);\n        _,_,y1,y2 = plt.axis()\n        nice()\n        if n>1:\n            bad = norm(emu,esig)\n            plt.plot(X, normalize(bad.pdf(X)),color='red',ls='dotted',label='ML')\n        shrange(m1,m2,d=Px.max()/50)\n        plt.legend()\n        plt.ylim(y1,y2)\n        plt.suptitle(f'N={n}')\n\n    if method=='good':\n        return m1, m2\n\n    raise NameError",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def genprob(mus,sigmas,n = 100):\n    mu1,mu2 = mus\n    sig1,sig2 = sigmas\n    x1 = mu1-3*sig2\n    x2 = mu2+3*sig2\n    \n    M = np.linspace(mu1,mu2,n)\n    S = np.linspace(sig1,sig2,n+1)\n    X = np.linspace(x1,x2,n+5)\n    \n    def gen():\n        mu =  np.random.rand()*(mu2-mu1) + mu1\n        sig = np.random.rand()*(sig2-sig1) + sig1\n        return mu,sig\n    return M,S,X,gen",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "M,S,X,gen = genprob((-1,1),(0.1,1))\ns,m = np.meshgrid(S,M,indexing='ij')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Vemos que la distribución predictiva efectivamente engloba una muestra futura la proporción de veces deseada, mientras que la gaussiana estimada con los parámetros más verosímiles lo hace con bastante menos frecuencia. La primera es más ancha, pero solo lo justo, sin pasarse.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "mu,sig = gen()\nBayesPredictInterval(mu + sig*G(3),show=True, truef=norm(mu,sig));",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "N = 5\ntot = 0\ntrials = 500\nfor k in range(trials):\n    mu,sig = gen()\n    dat = mu + sig*G(N)\n    x1,x2 = BayesPredictInterval(dat)\n    new = mu + sig*G(1)[0]\n    tot += x1 < new < x2\nprint(f'{tot/trials*100:.0f}%')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "N = 5\ntot = 0\ntrials = 500\nfor k in range(trials):\n    mu,sig = gen()\n    dat = mu + sig*G(N)\n    x1,x2 = BayesPredictInterval(dat,method='bad')\n    new = mu + sig*G(1)[0]\n    tot += x1 < new < x2\nprint(f'{tot/trials*100:.0f}%')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "N = 2\ntot = 0\ntrials = 500\nfor k in range(trials):\n    mu,sig = gen()\n    dat = mu + sig*G(N)\n    x1,x2 = BayesPredictInterval(dat)\n    new = mu + sig*G(1)[0]\n    tot += x1 < new < x2\nprint(f'{tot/trials*100:.0f}%')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "N = 2\ntot = 0\ntrials = 500\nfor k in range(trials):\n    mu,sig = gen()\n    dat = mu + sig*G(N)\n    x1,x2 = BayesPredictInterval(dat,method='bad')\n    new = mu + sig*G(1)[0]\n    tot += x1 < new < x2\nprint(f'{tot/trials*100:.0f}%')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Y para nota: una sola observación:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "N = 1\ntot = 0\ntrials = 500\nfor k in range(trials):\n    mu,sig = gen()\n    dat = mu + sig*G(N)\n    x1,x2 = BayesPredictInterval(dat)\n    new = mu + sig*G(1)[0]\n    tot += x1 < new < x2\nprint(f'{tot/trials*100:.0f}%')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "mu,sig = gen()\nBayesPredictInterval(mu + sig*G(1),show=True, truef=norm(mu,sig));",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Podríamos hacer lo siguiente: suponer una media de cero y una sigma de 0.5, que son los valores medios de la información a priori para formar el intervalo. Pero no funciona:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "N = 1\ntot = 0\ntrials = 500\nfor k in range(trials):\n    mu,sig = gen()\n    dat = mu + sig*G(N)\n    x1,x2 = 0 - 1.96*0.5, 0 + 1.96*0.5\n    new = mu + sig*G(1)[0]\n    tot += x1 < new < x2\nprint(f'{tot/trials*100:.0f}%')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Tampoco usando como centro la muestra observada:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "N = 1\ntot = 0\ntrials = 500\nfor k in range(trials):\n    mu,sig = gen()\n    dat = mu + sig*G(N)\n    med = np.mean(dat)\n    x1,x2 = med - 1.96*0.5, med + 1.96*0.5\n    new = mu + sig*G(1)[0]\n    tot += x1 < new < x2\nprint(f'{tot/trials*100:.0f}%')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Creo que no tiene mucho sentido evaluar esto con más de una muestra futura. Lo que se consiga será una \"amplificación\" de la calidad del intervalo, que es lo que queremos medir. Y estoy pensando que esto se puede hacer analíticamente. En lugar de sacar una nueva, ver la cantidad de la real que hay en el intervalo. Promediando esto en varias realizaciones posiblemente dé el mismo resultado.\n\nCuando hay sucesivas muestras futuras se podrían ir incorporando al estimador.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Jeffreys",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "El problema precioso que sale en su libro: probabilidad de que $x_3$ caiga entre $x_1$ y $x_2$ (es 1/3), y de que $\\mu$ caiga entre $x_1$ y $x_2$ (es 1/2).",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\nx1,x2,x3 = np.random.randn(3,10000)\nmn = np.min(np.array([x1,x2]),axis=0)\nmx = np.max(np.array([x1,x2]),axis=0)\n\nprint( ((mn<0) & (0<mx)).mean() )\n\nprint( ((mn<x3) & (x3<mx)).mean() ) ",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\nx1,x2,x3 = np.random.rand(3,10000)\nmn = np.min(np.array([x1,x2]),axis=0)\nmx = np.max(np.array([x1,x2]),axis=0)\n\nprint( ((mn<0.5) & (0.5<mx)).mean() )\n\nprint( ((mn<x3) & (x3<mx)).mean() )\n\nprint( (x3 > mx).mean() )\n\nprint( (x3 < mn).mean() )",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "No depende de la densidad concreta (eso sí, la localización debe ser la mediana). Vale para cualquier *location-scale parametric family*.\n\nEsto le complica mucho la vida a los frecuentistas ya que no pueden definir distribuciones predictivas.\n\nEl primer caso, que el \"true value\" esté entre las dos muestras creo que es inmediato cuando el \"true value\" es la mediana, ya que por definición tienes 1/2 de probabilidad a cada lado. Hay 4 casos equiprobables para las dos muestras de caer a cada lado de la mediana, de las cuales 2 la engloban.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Regresión",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Estamos interesados en ganar intuición en la selección Bayesiana de modelos. Las técnicas de grid son una buena herramienta para ello, especialmente en el problema de regresión con modelos polinomiales de diferentes grados.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "https://en.wikipedia.org/wiki/Bayes_factor",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Una conclusión importante obtenida *the hard way* es que para que el contraste de modelos sea correcto hay que tener en cuenta explícitamente las probabilidades *a priori* de los parámetros. Si añades parámetros pero cambias el rango de alguno de los anteriores el espacio explorado cambia en dos sentidos. Por un lado aumenta la dimensión, pero a la vez puede aumentar o disminuir el tamaño de las capas.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom numpy import log\n\ndef normalize(x):\n    return x/x.sum()\n\ndef lgauss1d(m, s, x):\n    return -0.5 * ((x-m)/s)**2 - log(s)\n\n%pip install -q https://raw.githubusercontent.com/albertoruiz/jupyterlite/main/content/misc/umucv-0.3-py3-none-any.whl\n\nimport umucv.prob as pr",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Datos",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(6,6))\nX = np.arange(8)\nY = 2 + 1/3*X + np.random.randn(len(X))/2\n\nplt.plot(X,Y,'.'); plt.axis('equal');",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Modelo lineal básico",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Creamos el bloque exhaustivo de parámetros con `meshgrid`.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "<p style='margin-left:2cm; color:#444; line-height:1.2'><small><small>**Nota**: El bloque se puede construir con bucles o *list comprehensions* pero numpy es mucho más rápido. Sin embargo, para ahorrar memoria, no merece la pena generar bloques replicados para los datos, que se suman inmediatamente.\n</small></small></p>",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "a1,a2 = -1  , 2\nb1,b2 = -1  , 5\ns1,s2 =  0.1, 2",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "n = 75\nA = np.linspace(a1,a2,n)\nB = np.linspace(b1,b2,n)\nS = np.linspace(s1,s2,n)\n\ndv = (a2-a1) * (b2-b1) * (s2-s1) / n**3\n\na,b,s = np.meshgrid(A,B,S, indexing='ij')\n\nL = 0\nfor x,y in zip(X,Y):\n    L += lgauss1d( a*x + b , s , y )\n\nL += -log(a2-a1) -log(b2-b1) -log(s2-s1)\n    \nP = normalize(np.exp(L))\n\nevi_lin = np.exp(L).sum() * dv",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Algunas distribuciones marginales:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,4))\nplt.subplot(1,3,1)\nplt.plot(A,P.sum(axis=(1,2)));\nplt.subplot(1,3,2)\nplt.plot(B,P.sum(axis=(0,2)));\nplt.subplot(1,3,3)\nplt.plot(S,P.sum(axis=(0,1)));",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.imshow(P.sum(axis=2),'gray');\nax = plt.gca()\nax.set_xticks([0,len(B)-1]); ax.set_xticklabels(B[[0,-1]]);\nax.set_yticks([0,len(A)-1]); ax.set_yticklabels(A[[0,-1]]);",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "El modelo más verosímil, marginalizando el ruido:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "am,bm,_ = [ x[k] for x,k in zip([A,B,S], np.unravel_index(np.argmax(L),L.shape)) ]\n\nx1 = min(X)-2\nx2 = max(X)+2\n\nplt.plot(X,Y,'.'); plt.axis('equal');\nplt.plot([x1,x2],[am*x1+bm,am*x2+bm]);",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Y un muestreo de los modelos:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def sample():\n    Pab = P.sum(axis=2)\n\n    v,p = zip(*np.ndenumerate(Pab))\n    lines = [(A[v[k][0]], B[v[k][1]]) for k in  np.random.choice(len(v),p=p,size=100)]\n\n    for a,b in lines:\n        plt.plot([x1,x2],[a*x1+b,a*x2+b],color='black', lw=3, alpha=0.05);\n    #plt.plot([x1,x2],[am*x1+bm,am*x2+bm],'yellow');\n    plt.plot(X,Y,'.',color='red'); plt.axis('equal');\n\nsample()",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Finalmente, la predicción en un $x$ concreto.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "YP = np.linspace(1,8,20)\n#u = np.ones(L.shape)\n#yp = np.outer(YP,u).reshape(*[-1]+list(u.shape))\nyp = YP.reshape(-1, *np.ones(L.ndim,int))\nxp = 8\n\nPP = normalize(np.exp((lgauss1d(a*xp +b , s , yp ) + L )))\nprint(PP.shape)\n\ndlin =  PP.sum(axis=(1,2,3))\n\nplt.plot(YP,dlin);\npr.showhdi(pr.P(dict(zip(YP,dlin))), 90)\nprint(am*xp+bm)",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<p style='text-align:right; margin-left:10cm; color:#444; line-height:1.2'><small><small>Creo que aquí no se puede hacer ningún atajo, necesitamos la expansión completa de todos los $y$ para poder normalizar y marginalizar correctamente. (Aunque tengo que comprobarlo, no estoy seguro.)\n</small></small></p>",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Modelo cuadrático",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "n = 50\n\nc1, c2 = -1/5 , 1/5\n\nA = np.linspace(a1,a2,n)\nB = np.linspace(b1,b2,n)\nS = np.linspace(s1,s2,n)\nC = np.linspace(c1,c2,n)\n\ndv = (a2-a1) * (b2-b1) * (s2-s1) * (c2-c1) / n**4\n\na,b,c,s = np.meshgrid(A,B,C,S, indexing='ij')\n\nL = 0\nfor x,y in zip(X,Y):\n    L += lgauss1d( c*x**2 + a*x + b , s , y )\n    \n# L*= 0  # to check prior\nL +=  -log(a2-a1) -log(b2-b1) -log(s2-s1) -log(c2-c1)    \n\nP = normalize(np.exp(L))\n\nevi_cua = np.exp(L).sum() * dv",
      "metadata": {
        "hidden": true,
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "evi_lin, evi_cua, evi_lin/evi_cua",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(16,4))\nplt.subplot(1,4,1)\nplt.plot(A,P.sum(axis=(1,2,3)));\nplt.subplot(1,4,2)\nplt.plot(B,P.sum(axis=(0,2,3)));\nplt.subplot(1,4,3)\nplt.plot(C,P.sum(axis=(0,1,3)));\nplt.subplot(1,4,4)\nplt.plot(S,P.sum(axis=(0,1,2)));",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.imshow(P.sum(axis=(2,3)),'gray');\nax = plt.gca()\nax.set_xticks([0,len(B)-1]); ax.set_xticklabels(B[[0,-1]]);\nax.set_yticks([0,len(A)-1]); ax.set_yticklabels(A[[0,-1]]);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "am,bm,cm,_ = [ x[k] for x,k in zip([A,B,C,S], np.unravel_index(np.argmax(L),L.shape)) ]\n\nplt.plot(X,Y,'.'); plt.axis('equal');\nxplot = np.linspace(x1,x2,50)\nyplot = [cm*x**2 + am*x + bm for x in xplot]\nplt.plot(xplot,yplot);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Y un muestreo de los modelos:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def sample():\n    Pabc = P.sum(axis=3)\n\n    v,p = zip(*np.ndenumerate(Pabc))\n    lines = [(A[v[k][0]], B[v[k][1]], C[v[k][2]]) for k in  np.random.choice(len(v),p=p,size=100)]\n\n    plt.figure(figsize=(4,6))\n    for a,b,c in lines:\n        yplot = [c*x**2 + a*x + b for x in xplot]\n        plt.plot(xplot,yplot,color='black', lw=3, alpha=0.05);\n    plt.plot(X,Y,'.',color='red'); plt.axis('equal');\n\nsample()",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "YP = np.linspace(1,8,20)\n#u = np.ones(L.shape)\n#yp = np.outer(YP,u).reshape(*[-1]+list(u.shape))\nyp = YP.reshape(-1, *np.ones(L.ndim,int))\nxp = 8\n\nPP = normalize(np.exp((lgauss1d(c*xp**2 + a*xp + b , s , yp ) + L )))\nprint(PP.shape)\n\ndcua =  PP.sum(axis=(1,2,3,4))\n\nplt.plot(YP, dcua);\npr.showhdi(pr.P(dict(zip(YP,dcua))), 90)\nprint(bm + am*xp + cm*xp**2)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.plot(YP,dlin, YP, dcua);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Modelo más simple",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "n = 120\n\nA = np.linspace(a1,a2,n)\nS = np.linspace(s1,s2,n)\n\ndv = (a2-a1) * (s2-s1) / n**2\n\na,s = np.meshgrid(A,S, indexing='ij')\n\nbf = 1\n\nL = 0\nfor x,y in zip(X,Y):\n    L += lgauss1d( a*x + bf , s , y )\n\nL +=  -log(a2-a1) -log(s2-s1)\n    \nP = normalize(np.exp(L))\n\nevi_simp = np.exp(L).sum() * dv",
      "metadata": {
        "hidden": true,
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "evi_lin, evi_simp, evi_lin/evi_simp",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(16,4))\nplt.subplot(1,4,1)\nplt.plot(A,P.sum(axis=(1)));\nplt.subplot(1,4,4)\nplt.plot(S,P.sum(axis=(0)));",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "am,_ = [ x[k] for x,k in zip([A,S], np.unravel_index(np.argmax(L),L.shape)) ]\n\nplt.plot(X,Y,'.'); plt.axis('equal');\nxplot = np.linspace(x1,x2,50)\nyplot = [am*x + bf for x in xplot]\nplt.plot(xplot,yplot); plt.grid();",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Y un muestreo de los modelos:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def sample():\n    Pa = P.sum(axis=1)\n\n    v,p = zip(*np.ndenumerate(Pa))\n    lines = [(A[v[k][0]]) for k in  np.random.choice(len(v),p=p,size=100)]\n\n    for a in lines:\n        yplot = [a*x + bf for x in xplot]\n        plt.plot(xplot,yplot,color='black', lw=3, alpha=0.05);\n    plt.plot(X,Y,'.',color='red'); plt.axis('equal');\n    \nsample()    ",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Función Q",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Eliminación analítica del parámetro $\\sigma$.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Muchos modelos consisten en una cierta ley que depende de parámetros $\\theta$, y las observaciones tienen un ruido gaussiano $\\sigma$ en principio desconocido. Si tenemos $N$ datos $D$, lo único importante es $Q(\\theta)$, la suma de errores cuadráticos. \n\n$$p(D|\\theta \\sigma) \\propto  \\frac{1}{\\sigma^N} \\exp\\left(-\\frac{Q(\\theta)}{2\\sigma^2}\\right)$$\n\nCon prior no informativa para $\\sigma$, Bayes nos dice:\n\n$$p(\\theta \\sigma|D) \\propto p(\\sigma) p(\\theta) p(D|\\theta \\sigma)  \\propto \\frac{1}{\\sigma}\\, p(\\theta)\\, \\frac{1}{\\sigma^N} \\exp\\left(-\\frac{Q(\\theta)}{2\\sigma^2}\\right) $$\n\nMarginalizando $\\sigma$ y evaluando la integral simbólica (p. ej. con sympy), llegamos a:\n\n$$p(\\theta |D) \\propto p(\\theta) \\int_0^\\infty d\\sigma  \\frac{1}{\\sigma^{N+1}} \\exp\\left(-\\frac{Q(\\theta)}{2\\sigma^2}\\right) \\propto p(\\theta) Q(\\theta)^{\\frac{-N}{2}}$$\n\n",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Para la comprobación hay que usar, por supuesto el prior no informativo para $\\sigma$. Si usamos el uniforme, como en ejemplos anteriores, se produce una leve discrepancia en las marginales. Lo que confirma la consistencia de las expresiones.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "La estimación de la distribución a posteriori de sigma no es exacta pero sí una buena aproximación, que mejora con $N$:\n\n$$\np(\\sigma | D) \\sim \\frac{1}{\\sigma^{N+1}} \\exp\\left(-\\frac{Q_{min}}{2\\sigma^2} \\right)$$",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "n = 50\n\nc1, c2 = -1/5 , 1/5\n\nA = np.linspace(a1,a2,n)\nB = np.linspace(b1,b2,n)\nS = np.linspace(s1,s2,n)\nC = np.linspace(c1,c2,n)\n\ndv = (a2-a1) * (b2-b1) * (s2-s1) * (c2-c1) / n**4\n\na,b,c,s = np.meshgrid(A,B,C,S, indexing='ij')\n\nL = 0\nfor x,y in zip(X,Y):\n    L += lgauss1d( c*x**2 + a*x + b , s , y )\n\nL +=  -log(a2-a1) -log(b2-b1) -log(s) -log(c2-c1)    \n\nP = normalize(np.exp(L))\n\nevi_cua = np.exp(L).sum() * dv",
      "metadata": {
        "hidden": true,
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "a,b,c = np.meshgrid(A,B,C, indexing='ij')\n\nQ = 0\nfor x,y in zip(X,Y):\n    Q += ( y - (c*x**2 + a*x + b) ) ** 2\n\nN = len(X)\n    \nlnp = -N/2 *log(Q)\nlnp += -log(a2-a1) -log(b2-b1) -log(c2-c1)    \nlnp -= lnp.max()\nP2  = normalize(np.exp(lnp))",
      "metadata": {
        "hidden": true,
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(16,4))\nplt.subplot(1,4,1)\nplt.plot(A,P.sum(axis=(1,2,3)),lw=5);\nplt.plot(A,P2.sum(axis=(1,2)));\nplt.subplot(1,4,2)\nplt.plot(B,P.sum(axis=(0,2,3)),lw=5);\nplt.plot(B,P2.sum(axis=(0,2)));\nplt.subplot(1,4,3)\nplt.plot(C,P.sum(axis=(0,1,3)),lw=5);\nplt.plot(C,P2.sum(axis=(0,1)));\nplt.subplot(1,4,4)\nplt.plot(S,normalize(P.sum(axis=(0,1,2))));\npS = normalize(1/S**(len(X)+1)*np.exp(-Q.min()/2/S**2))\nplt.plot(S, pS);",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(8,4))\nplt.subplot(1,2,1)\nplt.imshow(1-P.sum(axis=(2,3)),'gray');\nax = plt.gca()\nax.set_xticks([0,len(B)-1]); ax.set_xticklabels(B[[0,-1]]);\nax.set_yticks([0,len(A)-1]); ax.set_yticklabels(A[[0,-1]]);\nplt.subplot(1,2,2)\nplt.imshow(1-P2.sum(axis=(2)),'gray');\nax = plt.gca()\nax.set_xticks([0,len(B)-1]); ax.set_xticklabels(B[[0,-1]]);\nax.set_yticks([0,len(A)-1]); ax.set_yticklabels(A[[0,-1]]);",
      "metadata": {
        "hidden": true,
        "hide_input": false,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "La coincidencia es perfecta en la marginalización de $\\sigma$, y la aproximación a $p(\\sigma|D)$ es muy buena.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Queda pendiente la selección de modelos usando esta técnica. Una forma sería elegir el $\\sigma$ más probable de cada modelo y eliminarlo del modelo. Por cierto, ¿el exponente es N+1? en el caso anterior ajusta mejor N-1.",
      "metadata": {
        "hidden": true
      }
    }
  ]
}