{
  "metadata": {
    "hide_input": false,
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<span style=\"font-size:200%\">Inferencia Bayesiana</span><br>\n<span style=\"color: gray\">dic 2019</span><br>\n[*Alberto Ruiz*](http://dis.um.es/profesores/alberto)",
      "metadata": {
        "lang": "es"
      }
    },
    {
      "cell_type": "markdown",
      "source": "La inferencia Bayesiana es una extensión del razonamiento lógico a situaciones con incertidumbre, en las que deseamos estimar magnitudes desconocidas a partir de información incompleta o ruidosa.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Motivación",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom matplotlib import rc\n#rc('text', usetex=True)\n#rc('font', size=14)\n\nG = np.random.randn\n\ndef show1ddata(x, sz=(8,0.5), **kwargs):\n    plt.figure(figsize=sz)\n    \n    options = { 'marker': 'x', 's': 60, 'alpha': 0.75, 'color':'blue' }\n    options.update(kwargs)\n    \n    plt.ylim(-1,1);\n    plt.scatter(x,x*0, zorder=5, **options);\n    \n    ax = plt.gca()\n    ax.spines['left'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_yticklabels([])\n    ax.set_yticks([])\n    ax.spines['bottom'].set_position('zero')\n    ax.spines['bottom'].set_color('gray')\n    \n    col = 'gray'\n    ax.tick_params(axis='x', colors=col)\n    \ndef shbracket(x, k=2):\n    m = x.mean()\n    s = x.std()\n    d = 0.6\n    plt.fill_between([m-k*s,m+k*s], [d,d], -d, alpha=0.2, color='green')",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Supongamos que queremos estimar los parámetros ($\\mu$, $\\sigma$) de una gaussiana 1D a partir de unas pocas muestras:",
      "metadata": {
        "hidden": true,
        "hide_input": true
      }
    },
    {
      "cell_type": "code",
      "source": "n = 10\n\nμ = 1\nσ = 0.5\n\ndata = μ  +  σ * G(n)\n#print(data)\nshow1ddata(data, color='blue', alpha=0.5)\nplt.xlim(-5,5);",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Lo usual es calcular la media y la desviación estándar. Cuanto mayor sea $n$ más se aproximarán a los valores reales.",
      "metadata": {
        "hidden": true,
        "hide_input": true
      }
    },
    {
      "cell_type": "code",
      "source": "print(f'mean={np.mean(data):.3f}, std={np.std(data):.3f}')",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "scrolled": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Con esos estadísticos podemos establecer una región que englobe a la mayoría de las muestras recogidas (y esperamos que también futuras, si $n$ no es muy pequeño).",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "show1ddata(data, color='blue', alpha=0.5)\nplt.xlim(-5,5);\nshbracket(data)",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Pero a veces los datos disponibles están contaminados con  \"outliers\", y lo que es peor, es posible que la proporción  $\\varepsilon$ de dichos outliers sea desconocida.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "noisydata = np.append(data,[-4.3, 2.2, 4.1])\n\nshow1ddata(noisydata, color='blue', alpha=0.5)\nplt.xlim(-5,5);",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "En estas condiciones la media y la dispersión pueden quedar muy distorsionadas y dejan de tener utilidad.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "print(f'mean={np.mean(noisydata):.3f}, std={np.std(noisydata):.3f}')",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "scrolled": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "show1ddata(noisydata, color='blue', alpha=0.5)\nplt.xlim(-5,5);\nshbracket(noisydata)",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Podríamos quitar \"a ojo\" las muestras que tengan pinta de ser \"malas\", pero esto es poco riguroso y arriesgado. Lo ideal sería obtener buenas estimaciones de $\\mu$ y $\\sigma$ resistentes a los outliers y además, nos gustaría cuantificar su precisión.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Consideremos ahora el problema de la regresión lineal. Tenemos un conjunto de medidas $(x_k,y_k)$ como las que se muestran en la figura siguiente, y sospechamos que obedecen una sencilla ley de tipo $y = a+bx$. La variable $x$ puede considerarse exacta (nosotros la controlamos), pero la variable $y$ presenta un cierto ruido de medida, que suponemos gaussiano pero de intensidad $\\sigma$ desconocida.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(6,6))\n\nα = 1/3\nβ = 2\nσ = 0.5\n\nX = np.arange(8)\nn = len(X)\nY = α*X + β + σ*G(n)\n\nplt.plot(X,Y,'o'); plt.axis('equal'); plt.xlabel('x'); plt.ylabel('y');",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "El modelo lineal obtenido por mínimos cuadrados (recta de regresión) es:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "A = np.vander(X,2)\nB = Y\nsol = np.linalg.lstsq(A,B,rcond=None)[0]\n\n#print(sol)\nmn,mx = X.min(), X.max()\nd = (mx-mn)/5\nxc = np.linspace(mn-d,mx+d,100)\nye = np.vander(xc,2) @ sol\n\n\nplt.plot(X,Y,'o'); plt.axis('equal'); plt.xlabel('x'); plt.ylabel('y');\nplt.plot(xc,ye,'r'); plt.title('modelo lineal');",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Si el número de datos disponible es pequeño el modelo obtenido será muy sensible a las posiciones observadas y, por tanto, poco fiable.\n\nAdemás, podemos plantearnos la posibilidad de que la relación entre las variables en realidad sea no lineal, por ejemplo del tipo $y = a + bx + cx^2 $.",
      "metadata": {
        "hidden": true,
        "hide_input": false
      }
    },
    {
      "cell_type": "code",
      "source": "A = np.vander(X,3)\nB = Y\nsol = np.linalg.lstsq(A,B,rcond=None)[0]\n\n#print(sol)\nmn,mx = X.min(), X.max()\nd = (mx-mn)/5\nxc = np.linspace(mn-d,mx+d,100)\nye = np.vander(xc,3) @ sol\n\n\nplt.plot(X,Y,'o'); plt.axis('equal'); plt.xlabel('x'); plt.ylabel('y');\nplt.plot(xc,ye,'r'); plt.title('modelo cuadrático');",
      "metadata": {
        "hidden": true,
        "hide_input": true,
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "El ruido de medida hace difícil distinguir las dos situaciones.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Regla de Bayes",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Este tipo de problemas debe abordarse con algún método de razonamiento capaz de manejar correctamente la incertidumbre. La [inferencia Bayesiana](https://en.wikipedia.org/wiki/Bayesian_inference) representa la información sobre las magnitudes desconocidas mediante distribuciones de probabilidad y aplica las reglas del cálculo de probabilidades para reducir adecuadamente su incertidumbre en base a la información aportada por los datos experimentales.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "$P(\\theta)$ es la incertidumbre inicial sobre la magnitud desconocida $\\theta$.\n\n$P(d \\mid \\theta)$ es el modelo de la observación. Para un valor concreto de $\\theta$, nos dice lo probable que es cada posible dato d.\n\n$P( d_o \\mid \\theta)$ es la \"verosimilitud\" de cualquier posible $\\theta$ para un dato concreto observado $d_o$. Es la misma expresión matemática anterior del modelo de observación, pero donde se fija el primer argumento a $d_o$, quedando una función únicamente de $\\theta$. \n\nLa regla de Bayes resuelve el problema:\n\n$$P(A , B) = P(A \\mid B)\\, P(B) = P(B\\mid A)\\, P(A)$$\n\n$$\\underbrace{P(\\,\\theta \\mid d_o\\,)}_{posterior} = \\underbrace{P(\\,d_o \\mid  \\theta\\,)}_{likelihood} \\;\\underbrace{P(\\,\\theta\\,)}_{prior}\\; \\underbrace{\\frac{1}{P(\\,d_o\\,)}}_{normalization}$$\n\nOlvidándonos por el momento de la normalización (que en casos sencillos se consigue dividiendo por la suma de todas las posibilidades) la distribución posterior es proporcional al producto del modelo y la distribución inicial.\n\n$$P(\\theta \\mid  d_o) \\propto P(d_o\\mid \\theta) \\; P(\\theta)$$\n\nEstos dos ingredientes son en principio sencillos de expresar, matemática o computacionalmente, teniendo en cuenta las características de cada problema concreto. Lo interesante de la expresión anterior es que obtiene automáticamente el \"modelo inverso\" de cualquier sistema partiendo simplemente del \"modelo directo\".",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Múltiples datos",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Cuando tenemos un conjunto $D$ de datos observados $d_k$ independientes, su verosimilitud es un producto\n\n$$P(D \\mid \\theta) = \\prod_k P(d_k \\mid \\theta)$$\n\npero computacionalmente es mejor tomar logaritmos, de modo que la distribución posterior queda:\n\n$$\\log P(\\theta \\mid D) = \\sum_k \\log P(d_k \\mid \\theta) + \\log p(\\theta) \\color{gray}{+ K} $$\n\nEl proceso de inferencia consiste \"simplemente\" en evaluar la expresión anterior en los datos $d_k$ observados, para todos los posibles valores de todos los parámetros (normalmente $\\theta$ tendrá varias componentes).",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Ejemplos",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "En el ejemplo de una población normal contaminada con outliers, un posible modelo de medida es una mezcla de una normal y una variable $R$ con mucha dispersión que genera los outliers con probabilidad $\\epsilon$. Los parámetros son $\\theta = (\\mu,\\sigma,\\epsilon)$.\n\n$$P(x\\mid \\mu,\\sigma,\\epsilon) = \\epsilon R(x) + (1-\\epsilon)\\mathcal N[\\mu, \\sigma](x)$$\n\nNuestro objetivo es encontrar $P(\\mu \\sigma \\epsilon \\mid D)$, y a partir de ahí cualquier marginal como $P(\\mu \\sigma \\mid D)$, o $P(\\sigma  \\mid D)$, etc.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "En el ejemplo de regresión, para una función lineal el modelo de medida puede ser el siguiente, con $\\theta=(a,b,\\sigma)$.\n\n$$ P(y \\mid x,a,b,\\sigma) = \\mathcal N[a+bx ,\\sigma](y) $$\n\nY para la función cuadrática $\\theta=(a,b,c,\\sigma)$, con\n\n$$ P(y \\mid x,a,b,c,\\sigma) = \\mathcal N[a+bx+cx^2 ,\\sigma](y)  $$",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Resultado",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "El resultado de la inferencia Bayesiana es la distribución posterior conjunta de todos los parámetros, que puede utilizarse para responder a diferentes preguntas.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Por ejemplo, en el problema de regresión se pueden plantear varios objetivos:\n\n- Por un lado, obtener información $P(ab\\mid D)$ sobre los parámetros del modelo lineal o $P(abc\\mid D)$ del cuadrático, y sobre la intensidad del ruido $P(\\sigma\\mid D)$.\n\n\n- Normalmente estaremos interesados en el resultado de una observación futura $x_n$, cuya incertidumbre expresamos como $P(y\\mid x_n)$ y que se calculará marginalizando los demás parámetros (distribución predictiva). Utilizar los más probables es una simplificación en la que perdemos información. Siempre que sea computacionalmente tratable es preferible un ataque Bayesiano completo.\n\n\n- Finalmente, nos gustaría determinar si realmente merece la pena complicarnos la vida con el modelo cuadrático. La selección Bayesiana de modelos es un método riguroso de control de capacidad, que es una condición necesaria para la generalización.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Hiperparámetros",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Puede ocurrir que tengamos un buen modelo de medida $p(D\\mid\\theta)$ pero que la información a priori sobre los parámetros sea también imprecisa (p.ej. se desconce su rango de valores posibles). Entonces se puede introducir información a priori sobre ellos, ampliando el modelo:\n\n$$p(d\\mid \\theta \\alpha) = p(d\\mid\\theta)$$\n\n$$p(\\theta\\alpha) = p(\\theta\\mid\\alpha)\\; p(\\alpha)$$\n\nEntonces\n\n$$p(d \\mid \\theta \\alpha) \\propto  p(d \\mid \\theta\\alpha)\\;p(\\theta\\alpha) = p(d\\mid\\theta)\\; p(\\theta \\mid \\alpha)\\; p(\\alpha) $$\n\nAl final se marginalizan los parámetros que no interesan:\n\n$$p(d\\mid\\theta) =  \\sum_\\alpha\\; p(d\\mid\\theta\\alpha)$$\n\nSiempre es necesario incluir explícitamente la información a priori sobre los parámetros o hiperparámetros para conseguir la distribución a posteriori. Es la característica fundamental de la inferencia Bayesiana. Y se considera algo muy positivo, frente a alternativas donde esta información está implícita en otras suposiciones, o que renuncian completamente a responder con una distribución de probabilidad sobre los parámetros de interés. ",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Parámetros auxiliares",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Los parámetros auxiliares (\"*nuisance*\") también se marginalizan. Esta es una ventaja importante del enfoque Bayesiano. Se introducen parámetros que facilitan la definición del modelo del sensor, y luego desaparecen teniendo en cuenta todas sus posibilidades adecuadamente ponderadas.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Técnicas",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Si las distribuciones son matemáticamente simples (p. ej. Gaussianas) la regla de Bayes se puede aplicar de forma analítica. Esto ocurre por ejemplo en el [Filtro de Kalman](Kalman.ipynb).\n\nSi no es así, pero el número de parámetros es pequeño se puede discretizar el dominio y evaluar explícitamente todas las posibilidades (técnicas de *grid*). Otra posibilidad es representar las distribuciones mediantes muestras (filtro de partículas).\n\nSi el número de parámetros mayor hay que recurrir a las técnicas [MCMC](MCMC.ipynb).",
      "metadata": {
        "hidden": true
      }
    }
  ]
}