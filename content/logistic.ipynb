{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Logistic Regression",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[wikipedia](https://en.wikipedia.org/wiki/Logistic_regression)\n\nEn principio no hay gran diferencia en la frontera de decisión final entre el ajuste lineal directo de las etiquetas +1 -1 con la pseudoinversa y la técnica más correcta de _iterative reweighted least squares_.\n\nLa diferencia está en que la regresión logística estima la probabilidad de las etiquetas. Se asume un experimento aleatorio, no un \"ruido de medida\".\n\nRecordemos que MSE lineal va más o menos bien a pesar de que penaliza etiquetas muy bien clasificadas dentro de la frontera de decisión. Por el contrario, ajustar bien la logistic regression es un proceso no lineal, y realmente obtiene las probabilidades de pertenencia, lo cual puede ser importante a la hora de predecir. No sólo te da la frontera, sino lo abrupta que es la transición de probabilidades.\n\nLa idea genial es estimar linealmente (o con un modelo lineal generalizado con features) los _logodds_.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "En este experimento vamos a generar un dataset artificial obteniendo $P(c=1) = \\sigma(w^Tx)$, donde $\\sigma(·)$ es la típica nolinealidad suave. En la zona de probabilidad intermedia las clases se entremezclan de forma aleatoria.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ninv = np.linalg.inv\npinv = np.linalg.pinv\n\ndef sigma(x):\n    return 1/(1+np.exp(-x))\n\n# dibuja una recta \"infinita\" dadas sus coordenadas homogéneas\ndef shline(l,xmin=-2000,xmax=2000, **kwargs):\n    a,b,c = l / np.linalg.norm(l)\n    if abs(b) < 1e-6:\n        x = -c/a\n        r = np.array([[x,xmin],[x,xmax]])\n    else:\n        y0 = (-a*xmin - c) / b\n        y1 = (-a*xmax - c) / b\n        r = np.array([[xmin,y0],[xmax,y1]])\n    plt.plot(*r.T, **kwargs)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "w = [2, -1, 1]\n#w = [1, -0.5, 1]\n#w = [20, -10, 1]\n\ndata = np.random.randn(1000,2)*2\n\n\nX = np.hstack([data, np.ones([len(data),1])])\nX.shape\nlogodds = X@w\n\nprobs = sigma(logodds)\n\ny = np.array([np.random.choice([1,0], p=[p,1-p]) for p in probs])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(6,6))\nplt.scatter(*data.T,10,y,cmap='coolwarm',alpha=1);\nplt.axis('equal')\nax = plt.axis()\nshline(w,color='black')\nplt.axis(ax);\nplt.show()\n\nplt.figure(figsize=(6,6))\nplt.scatter(*data[y==0].T,10,'blue');\nshline(w,color='black')\nplt.axis(ax);\nplt.show()\n\nplt.figure(figsize=(6,6))\nplt.scatter(*data[y==1].T,10,'red');\nshline(w,color='black')\nplt.axis(ax);\nplt.show()\n\nplt.figure(figsize=(6,6))\nplt.scatter(*data.T,10,logodds,cmap='coolwarm',alpha=1,vmin=-3,vmax=3);\nshline(w,color='black')\nplt.axis(ax)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Vamos a intentar estimar los parámetros de modelo mediante el algoritmo iterativo:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "wlr = np.random.rand(3)/10\n\nfor _ in range(15):\n    mu =  sigma(X@wlr)\n    S = np.diag( mu*(1-mu) )\n    wlr = inv(X.T @ S @ X)@X.T@(S@X@wlr + y - mu)\n    print(wlr)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "La solución obtenida se parece mucho a la verdadera:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "wye = pinv(X)@(2*y-1)\nprint(wye)\nwye/abs(wye[-1])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "wmse = pinv(X)@logodds\nwmse",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(6,6))\nplt.scatter(*data.T,10, X@wlr ,cmap='coolwarm',alpha=1,vmin=-3,vmax=3);\nax = plt.axis()\nshline(w)\n#shline(wmse,color='gray')\nshline(wlr,color='green')\nshline(wye,color='blue')\nshline(wlr+[0,0,3],color='green',lw=0.5)\nshline(wlr+[0,0,-3],color='green',lw=0.5)\nshline(np.array(w)+[0,0,3],color='gray')\nshline(np.array(w)+[0,0,-3],color='gray')\n\nplt.axis(ax);",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "La estimación completa del modelo logístico nos devuelve también el ancho de la franja donde hay incertidumbre en la clasificación. En este ejemplo hemos puesto las fronteras con un logodds de $\\pm 3$ que corresponde con una probabilidad de",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sigma(3)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}