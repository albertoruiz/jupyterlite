{
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Filtro de Kalman\n<span style=\"color: gray\">dic 2019</span><br>\n[*Alberto Ruiz*](http://dis.um.es/profesores/alberto)",
      "metadata": {
        "lang": "es"
      }
    },
    {
      "cell_type": "markdown",
      "source": "La inferencia Bayesiana puede realizarse de forma analítica cuando las variables involucradas son [normales](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) (gaussianas).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Modelo gaussiano",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Las operaciones de marginalización, condicionamiento y conjunción pueden calcularse de forma analítica cuando las variables son normales.\n\nSon el fundamento del filtro de Kalman y de los [procesos gaussianos](https://en.wikipedia.org/wiki/Gaussian_process), entre otras muchas aplicaciones.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "El [artículo de wikipedia](https://en.wikipedia.org/wiki/Kalman_filter) está bastante bien. Puede ser útil repasar los apartados C.5 y C.6 de mis [apuntes](http://dis.um.es/profesores/alberto/material/percep.pdf). ",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Expresiones analíticas",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Si reordenamos las variables en dos grupos $(\\boldsymbol x, \\boldsymbol y)$ la densidad conjunta se puede expresar en función de las medias y matrices de covarianza de cada grupo y la covarianza cruzada:\n\n$$ \n\\newcommand{\\mat}[1]{\\boldsymbol{\\mathtt #1}}\n\\newcommand{\\T}{^\\mathsf T}\n\\newcommand{\\vec}[1]{\\boldsymbol{#1}}\n\\newcommand{\\I}{^{-1}}\np(\\vec x, \\vec y) \\sim \\mathcal N\\left(\\begin{bmatrix}\\vec \\mu_x \\\\ \\vec \\mu_y\\end{bmatrix}, \\begin{bmatrix}\\Sigma_{xx} & \\Sigma_{xy} \\\\ \\Sigma_{yx} & \\Sigma_{yy} \\end{bmatrix}\\right) = \\mathcal N \\left(\\begin{bmatrix}\\vec a \\\\ \\vec b\\end{bmatrix}, \\begin{bmatrix}\\mat A & \\mat C\\T \\\\ \\mat C & \\mat B \\end{bmatrix}\\right) $$\n\n\nLa densidad **marginal** de cualquier grupo se obtiene simplemente seleccionando las variables deseadas tanto en la media como en la matriz de covarianza. Por ejemplo:\n\n$$p(\\vec y) \\sim  \\mathcal N \\left(\\vec b, \\mat B\\right) $$\n\n\nLa densidad de un grupo de variables **condicionada** a la observación de otro grupo de variables es también gaussiana y se puede expresar de la siguiente forma:\n\n$$\np(\\vec y \\mid \\vec x) \\sim \\mathcal N \\left(\\vec b + \\mat C \\mat A\\I (\\vec x - \\vec a)\\; , \\; \\mat B - \\mat C \\mat A\\I \\mat C\\T\\right)\n$$\n\n(La media condicionada de esta gaussiana es la recta de regresión lineal).\n\nEn ocasiones estamos interesados realizar inferencia sobre unas variables $\\vec x$ a partir de la observación de una cierta función de ellas: $\\vec y = f(\\vec x)$. Si $\\vec x$ es gaussiana y la función $f$ es lineal podemos obtener fácilmente la densidad **conjunta** $p(\\vec x,\\vec y)$, que también es gaussiana, y realizar el condicionamiento como se acaba de explicar.\n\nConcretamente, sea $p(\\vec x) \\sim \\mathcal N (\\vec \\mu, \\mat P)$ y $f(\\vec x) = \\mat H \\vec x$ con ruido gaussiano aditivo de media $\\vec o$ y covarianza $\\mat R$. Esto significa que $p(\\vec y| \\vec x) \\sim \\mathcal N(\\mat H \\vec x + \\vec o, \\mat R)$. Entonces la densidad conjunta es:\n\n$$\np(\\vec x, \\vec y) \\sim \\mathcal N\\left(\\begin{bmatrix}\\vec \\mu \\\\ \\mat H \\vec \\mu + \\vec o\\end{bmatrix}, \\begin{bmatrix}\\mat P & \\mat P \\mat H\\T \\\\ \\mat H \\mat P & \\mat H \\mat P \\mat H\\T + \\mat R\\end{bmatrix}\\right)\n$$\n\n\nY la densidad condicionada contraria $p(\\vec x \\mid \\vec y)$ es:\n\n$$p(\\vec x \\mid \\vec y) \\sim \\mathcal N \\left(\\vec \\mu + \\mat K (\\vec y - \\mat H \\vec \\mu - \\vec o) , (\\mat I - \\mat K \\mat H )\\mat P \\right)$$\n\ndonde\n\n$$ \\mat K= \\mat P \\mat H\\T (\\mat H \\mat P \\mat H\\T + \\mat R)\\I $$\n\nEsta expresión está construida de manera que a partir de la observación $\\vec y$\ncorregimos la información sobre $\\vec x$ con una \"ganancia de Kalman\" $\\mat K$ que depende\ndel balance entre la incertidumbre a priori $\\mat P$, el ruido de la medida $\\mat R$, y el modelo de medida $\\mat H$.\n\n\nOtra forma de verlo: la densidad conjunta se puede expresar de dos formas: modelo de medida $\\times$ prior = posterior $\\times$ evidencia\n\n$$p(\\vec y \\mid \\vec x) \\; p(\\vec x)  =  p(\\vec x \\mid \\vec y) \\;  p(\\vec y) $$\n\n$$\\mathcal N (\\vec y \\mid \\mat H \\vec x + \\vec o, \\mat R) \\;\n\\mathcal N (\\vec x \\mid \\vec \\mu, \\mat P) =\n\\mathcal N (\\vec x \\mid \\vec \\eta_{\\vec y}, \\mat Q) \\;\n\\mathcal N (\\vec y \\mid \\mat H \\vec \\mu + \\vec o, \\mat H \\mat P \\mat H\\T + \\mat R)$$\n\n\nLa incertidumbre inicial sobre $\\vec x$ era $\\mat P$, que se reduce a $\\mat Q$ tras la observación de $\\vec y$:\n\n$$ \\mat Q\\I = \\mat P\\I + \\mat H\\T \\mat R\\I \\mat H $$\n\nY el estimador de $\\vec x$ se actualiza de $\\vec \\mu$ a $\\vec \\eta _ {\\vec y}$, que puede expresarse como una combinación ponderada de la observación y la información a priori:\n\n$$\\vec \\eta _ {\\vec y} = (\\mat Q \\mat H\\T \\mat R\\I) (\\vec y -\\vec o) + (\\mat Q \\mat P\\I) \\vec \\mu $$\n\nLa \"evidencia\" $p(\\vec y)$ es la verosimilitud de la medida $\\vec y$ teniendo en cuenta todos los posibles $\\vec x$ (convolución de dos gaussianas). Juega un papel esencial en la selección de modelos.\n",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Experimentos",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Para hacer experimentos usaremos una implementación de estas operaciones disponible en `umucv.prob`.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom   mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\n%pip install -q https://raw.githubusercontent.com/albertoruiz/jupyterlite/main/content/misc/umucv-0.3-py3-none-any.whl\n\nfrom umucv.prob import G",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "En primer lugar veremos un ejemplo muy simple. Considera la siguiente gaussiana de dos componentes, cuya densidad se muestra como una superficie en 3D y como una elipse de incertidumbre.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "g = G([2,3], [[4,3],\n              [3,4]])\n\nfig = plt.figure(figsize=(10,5))\nx = np.linspace(-3,7,50)\ny = np.linspace(-2,8,50)\nx1,x2 = np.meshgrid(x,y)\ngxy = g.logprob()\nz = np.array([[np.exp(gxy(np.array([x,y]))) for x in x] for y in y])\n\nax = fig.add_subplot(121, projection='3d')\nax.plot_surface(x1,x2,z, cmap='coolwarm', linewidth=0.5, rstride=2, cstride=2);\nax.view_init(elev=40,azim=90)\nax.set_xlabel('x'); ax.set_ylabel('y'); ax.set_title('p(x,y)'); ax.set_zticks([])\n\nplt.subplot(1,2,2)\nplt.plot(*g.ellipse().T);\nplt.plot(*g.m,'.');\nplt.xlabel('x'); plt.ylabel('y'); plt.title('elipse de incertidumbre'); plt.axis('equal');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "En el siguiente diagrama se muestra la densidad conjunta $p(x,y)$, las dos densidades marginales, y la densidad de $x$ condicionada a un valor de y.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(5,5))\nplt.plot(*g.ellipse().T, label='$p(x,y)$');\npx = g.marg([0]).logprob()\npy = g.marg([1]).logprob()\nplt.plot(x, [-3+10*np.exp(px(x)) for x in x], label='p(x)');\nplt.plot([-4+10*np.exp(py(y)) for y in y], y, label='p(y)');\ngx = g.cond([6]).logprob()\nplt.plot(x, [-3+10*np.exp(gx(x)) for x in x], label='p(x|y=6)');\nplt.plot([-4,7],[6,6],ls='dashed',color='gray');\nplt.xlabel('x'); plt.ylabel('y'); plt.axis('equal');\nplt.legend(loc=(1.04,0), fontsize=15);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "La misma situación en 3D, donde se muestra el corte producido por la observación $y=5$ en la densidad conjunta, (verosimilitud o likelihood), y la probabilidad condicionada, que es simplemente la normalización del corte.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "fig = plt.figure(figsize=(6,5))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(x1,x2,z, cmap='coolwarm', linewidth=0.5, rstride=1, cstride=1);\nax.view_init(elev=50,azim=60)\nax.set_xlabel('x'); ax.set_ylabel('y'); ax.set_title('p(x,y)'); ax.set_zticks([])\n\n\nyobs = 5\n\nz6 = [np.exp(gxy(np.array([x,yobs]))) for x in x]\n\nax.plot3D(x, yobs+x*0, z6, label=\"$p(x , y=5)$\");\nax.plot3D(x, x*0+8, [1/5*np.exp(gx(x)) for x in x],label=\"$p(x\\mid y=5)$\");\nplt.legend();",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Veamos ahora la densidad conjunta de una variable $x\\sim \\mathcal N[0,2]$ y una función lineal de ella $y = 2x + 5 + \\mathcal N[0,1]$:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "g = G([0],[[4]]).jointLinear([[2]], G([5], [[1]]))\n                            \nplt.figure(figsize=(5,5))\nplt.plot(*g.ellipse().T); plt.axis('equal'); plt.grid(ls='dotted');\nplt.xlabel('x'); plt.ylabel('y');\nprint(g.m)\nprint(g.c)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Calculamos la densidad condicionada a y=0 a partir de la densidad conjunta:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "# se condiciona los últimos elementos del vector\ng = g.cond([0])\ng.m, g.c",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Y hacemos lo mismo con la fórmula directa que usa la ganancia K, sin pasar por la densidad conjunta:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "g = G([0],[[4]]).bayesGaussianLinearK([[2]], G([5],[[1]]), [0])\ng.m, g.c",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Veamos ahora un caso más interesante, donde $\\vec x$ tiene dos componentes, que observamos con ruido.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "g = G([0,0] , [[4,3],\n               [3,4]])\n\nplt.figure(figsize=(5,5))\nplt.plot(*g.ellipse().T); plt.axis('equal'); plt.grid(ls='dotted');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "La densidad conjunta \"estado\"-\"observación\" es de dimensión 4.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "error = np.diag([0.4,0.1])\ng.jointLinear(np.eye(2), G([0,0], error)).c",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Dada una observación reducimos la incertidumbre:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "obs = [0,-3]\nplt.figure(figsize=(5,5))\nplt.axis('equal'); \npost = g.bayesGaussianLinear(np.eye(2), G([0,0], error),  obs )\nplt.plot(*g.ellipse().T, label='prior'); \nplt.plot(*post.ellipse().T, label='posterior', color='blue');\nplt.plot(*G(obs,  error).ellipse().T, label='likelihood');\nplt.grid(ls='dotted'); plt.legend(loc=(1.04,0));",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "El siguiente ejemplo es la clave del filtro de Kalman: el estado tiene dos variables $(x,y)$, pero la observación es incompleta $z=x+y$ y ruidosa. También reduce la incertidumbre sobre el estado:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(5,5))\npost = g.bayesGaussianLinear([[1,1]], G([0], [[0.4]]),  [2] )\nplt.plot(*g.ellipse().T, label='prior'); \nplt.plot(*post.ellipse().T, label='posterior', color='blue');\nplt.grid(ls='dotted'); plt.legend(loc=(1.04,0));\nplt.plot([-2,4],[4,-2],color='gray',ls='dashed');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Implementación",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Una implementación sencilla del filtro de Kalman y del UKF está disponible en el módulo `umucv.kalman`.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "%matplotlib inline\n\nimport numpy as np\nimport numpy.linalg as la \nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\n\nfrom umucv.kalman import kalman, ukf\nimport cv2\nimport umucv.htrans as ht\n\ndegree = np.pi/180",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Por comodidad la incluímos aquí:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "def mikalman(mu,P,F,Q,B,u,z,H,R):\n    # mu, P : estado actual y su incertidumbre\n    # F, Q  : sistema dinámico y su ruido\n    # B, u  : control model y la entrada\n    # z     : observación\n    # H, R  : modelo de observación y su ruido\n    \n    mup = F @ mu + B @ u;\n    pp  = F @ P @ F.T + Q;\n\n    zp = H @ mup\n\n    # si no hay observación solo hacemos predicción \n    if z is None:\n        return mup, pp, zp\n\n    epsilon = z - zp\n\n    k = pp @ H.T @ np.linalg.inv(H @ pp @ H.T +R)\n\n    new_mu = mup + k @ epsilon;\n    new_P  = (np.eye(len(P))-k @ H) @ pp;\n    return new_mu, new_P, zp",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Ilustración 1-D",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Vamos a resolver un problema sintético en el que un objeto se mueve en una única dimensión $x$ con una aceleración constante $a$. Desconocemos la velocidad inicial $v_0$ y solo observamos su posición a lo largo del tiempo, contaminada con ruido gaussiano de desviación $\\sigma_r$.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "v0 = 0.5\na = -0.005\ndt = 1\nt = np.arange(0,100,dt)\n\nsigmaR = 1\nzp = v0*t + 1/2*a*t**2\nzs = zp + sigmaR*np.random.randn(len(t));\n\nplt.plot(t,zs);\nplt.title(\"observaciones ruidosas de la posición\"); plt.xlabel(\"t\"); plt.ylabel(\"x\");",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "El modelo del sistema es el siguiente:\n\n$$\n\\begin{align*}\nx_{k+1} &= x_k + \\Delta t \\; v_k \\\\\nv_{k+1} &= v_k \n\\end{align*}\n$$\n\nSuponiendo $\\Delta t=1$, lo expresamos en forma matricial como transformaciones lineales del vector de estado:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "# modelo de evolución del sistema\nf = np.array(\n    [[1, dt],\n     [0,  1]])\n\n# control\nB = np.array([[dt**2/2],[dt]])\nu = np.array([a])\n\n# el ruido del proceso se puede poner como incertidumbre en la aceleración\nsigmaa = np.array([[abs(a/100)]])\ns = B @ sigmaa**2 @ B.T",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#modelo de la observación\nH = np.array([[1,0]])\n\n#y su ruido\nr = np.array([[sigmaR**2]])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#el estado inicial\nmu0 = np.array([5,0])\n\np0 = np.array(\n    [[100000, 0],\n     [0, 100000]])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Calculamos la estimación del vector de estado (posición y velocidad) para cada nueva observación:",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "mu = mu0\np = p0\n\nres = np.array([[mu[0],mu[1],np.sqrt(p[0,0]),np.sqrt(p[1,1])]])\n\nfor z in zs:\n    mu,p,_ = kalman(mu,p,f,s,B,u,z,H,r)\n    res = np.append(res,[[mu[0],mu[1],np.sqrt(p[0,0]),np.sqrt(p[1,1])]],axis=0)\n\nprint(res[:10])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(res[-5:])",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.plot(t,zs,t,zp);\nplt.plot(t,res[1:,0],t,res[1:,0] + 2*res[1:,2],t,res[1:,0] - 2*res[1:,2],color='red');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.plot(t,zs,t,zp,t,res[1:,0]);",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Predicción sin observación: en un momento dado se pierden las medidas y la estimación se hace \"a ciegas\".",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "mu = mu0\np = p0\n\nres = np.array([[mu[0],mu[1],np.sqrt(p[0,0]),np.sqrt(p[1,1])]])\n\nran = 50\n\nfor z in zs[:ran]:\n    mu,p,_ = kalman(mu,p,f,s,B,u,z,H,r)\n    res = np.append(res,[[mu[0],mu[1],np.sqrt(p[0,0]),np.sqrt(p[1,1])]],axis=0)\n\nfor z in zs[ran:]:\n    mu,p,_ = kalman(mu,p,f,s,B,u,None,H,r)\n    res = np.append(res,[[mu[0],mu[1],np.sqrt(p[0,0]),np.sqrt(p[1,1])]],axis=0)\n\n    \n# extraemos las varianzas de la estimación de posición\n# para dibujar la banda de incertidumbre\nplt.plot(t[:ran],zs[:ran],t,zp);\nplt.plot(t,res[1:,0],t,res[1:,0] + 2*res[1:,2],t,res[1:,0] - 2*res[1:,2],color='red');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Evolución de las elipses de incertidumbre.",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "from umucv.prob import G\n\ng0 = G(np.array([5,0]), \n       np.array([[100, 0],\n                 [0, 100]]))\n\ndef showkalman2(g,z):\n    g1 = G( f @ g.m + B @ u  ,    f @ g.c @ f.T + s );\n    m,c,_ = kalman(g.m,g.c,f,s,B,u,z,H,r)\n    g2 = G(m,c)\n    plt.plot(*g1.ellipse().T,color='green');\n    if z is not None:\n        plt.plot(*g2.ellipse().T,color='blue');\n    return g2\n\nplt.figure(figsize=(10,5))\nplt.plot(*g0.ellipse().T,color='orange');\ng = g0\nfor z in [12,23,36,47,None,None]:\n    g = showkalman2(g,z)\nplt.axis('equal'); plt.xlabel('x'); plt.ylabel('v');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Tiro parabólico 2D",
      "metadata": {
        "heading_collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "[Tiro parabólico](http://hyperphysics.phy-astr.gsu.edu/hbase/traj.html#tra6).\n\n",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "TFG de Pablo Saura (UMU, 2017).\n\n- [vídeo 1](https://www.youtube.com/watch?v=MxwVwCuBEDA)\n\n- [vídeo 2](https://www.youtube.com/watch?v=YlPOTxYvt6U)",
      "metadata": {
        "hidden": true
      }
    },
    {
      "cell_type": "code",
      "source": "x0 = np.array([0,0])\nangle = 80*degree\nv0 = 10*np.array([np.cos(angle), np.sin(angle)])\n\na = np.array([0, - 9.8])\nt = np.arange(0,2.01,0.1)\n\nZ = xp,yp = ht.col(x0)  +  ht.col(v0)* ht.row(t) + 1/2 * ht.col(a) * ht.row(t**2)\n\nplt.figure(figsize=(8,6))\nplt.plot(xp,yp,'.-',markersize=10);\nplt.axis('equal'); plt.grid(); plt.xlabel('$x$ (m)'); plt.ylabel('$y$ (m)');\nplt.title('trayectoria ideal ($\\Delta t = 0.1s$)');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "fps = 25\ndt  = 1/fps\n\nF = np.array(\n    [1, 0,  dt,  0,  \n     0, 1,  0, dt,  \n     0, 0,  1,  0,  \n     0, 0,  0,  1 ]).reshape(4,4)\n\n\nB = np.array(\n         [dt**2/2, 0,       \n          0,       dt**2/2, \n          dt,      0,       \n          0,       dt      ]).reshape(4,2)\n\n\nH = np.array(\n    [1,0,0,0,\n     0,1,0,0]).reshape(2,4)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def evol(x,u):\n    return F@x + B@u",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "s0 = np.hstack([x0,v0])\n\nr = [s0]\ns = s0\nfor k in range(round(2/dt)):\n    s = evol(s,a)\n    r.append(s)\n    #print(s)\nr = np.array(r)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "xt,yt,*vs = r.T",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(8,6))\nplt.plot(xt,yt,'.-',markersize=10); plt.axis('equal'); plt.grid();\nplt.xlabel('$x$ (m)'); plt.ylabel('$y$ (m)');\nplt.title('comprobación del modelo ($\\Delta t = (1/25) s$)');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "x0 = np.array([0,0])\nangle = 80*degree\nv0 = 10*np.array([np.cos(angle), np.sin(angle)])\n\na = np.array([0, - 9.8])\nt = np.arange(0,2.01,dt)\n\nnoise = 0.05\n\n# trayectoria verdadera (desconocida en realidad)\nZt = xt,yt = ht.col(x0)  +  ht.col(v0)* ht.row(t) + 1/2 * ht.col(a) * ht.row(t**2)\n\n# trayectoria observada\nZ  = xo,yo = Zt + noise*np.random.randn(2,len(t))\n\nplt.figure(figsize=(8,6))\nplt.plot(xo,yo,'.-',markersize=10);\nplt.axis('equal'); plt.grid(); plt.xlabel('$x$ (m)'); plt.ylabel('$y$ (m)');\nplt.title('trayectoria observada ($\\Delta t = (1/25) s$)');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# estado que Kalman va actualizando. Este es el valor inicial\n\n             # x, y, vx, vy\nmu = np.array([0,0,0,0])\n            # sus incertidumbres \nP  = np.diag([100,100,100,100])**2\n#res = [(mu,P,mu)]\nres=[]\nN = 15  # para tomar un tramo inicial y ver qué pasa si luego se pierde la observación\n\nsigmaM = 0.001   # ruido del modelo\nsigmaZ = 3*noise  # debería ser igual al ruido de media del proceso de imagen. 10 pixels pje.\n\nQ = sigmaM**2 * np.eye(4)\nR = sigmaZ**2 * np.eye(2)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "                                    # z es la medida del centro de la pelota observada\n                                    # mu es la estimación filtrada actualizada\nfor z in Z.T[1:N]:\n    mu,P,pred = kalman(mu,P,F,Q,B,a,z,H,R)\n    res += [(mu,P,pred)]\n\nfor _ in range(50-N):\n    mu,P,pred = kalman(mu,P,F,Q,B,a,None,H,R)  # aquí solo continuamos la predicción\n    res += [(mu,P,pred)]\n    \n\nres = np.array(res)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "xe = [mu[0] for mu,_,_ in res]             # coordenada x estimada\nxu = [2*np.sqrt(P[0,0]) for _,P,_ in res]  # su incertidumbre\n\nye = [mu[1] for mu,_,_ in res]             # lo mismo para y\nyu = [2*np.sqrt(P[1,1]) for _,P,_ in res]",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "fig,ax = plt.subplots(figsize=(8,8))\n\nfor k in range(len(xe)):\n    ax.add_patch(Ellipse(xy=(xe[k],ye[k]), width=xu[k], height=yu[k], angle = 0, alpha=0.2))\n\nplt.plot(xe,ye,lw=1)\nplt.plot(xo[1:N],yo[1:N])#,xe,ze);\nplt.plot(xt,yt,lw=0.5,color='gray'); plt.grid()\nplt.axis('equal');",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from matplotlib import animation, rc\nfrom IPython.display import HTML\nrc('animation', html='html5')",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize=(8,8))\nplt.close();\nax.set_xlim(( -1, 4))\nax.set_ylim(( 0, 5))\n\nline1, = ax.plot([], [], lw=1)\nline2, = ax.plot([],[])\nline3, = ax.plot([],[],'.',markersize=15)\nline4, = ax.plot([],[])\nline5, = ax.plot([],[])\nline6, = ax.plot(xt,yt,lw=0.5,color='gray')\n\nmu0 = np.array([0,0,0,0])\n            # sus incertidumbres \nP0  = np.diag([10,10,10,10])**2\n#res = [(mu,P,mu)]\n\ndef animate(i):\n    global mu,P\n    N = i\n    res=[]\n    mu = mu0\n    P  = P0\n    for z in Z.T[1:N]:\n        mu,P,pred = kalman(mu,P,F,Q,B,a,z,H,R)\n        #print(mu)\n        res += [(mu,P,pred)]\n\n    for _ in range(len(Z.T)-N):\n        mu,P,pred = kalman(mu,P,F,Q,B,a,None,H,R)  # aquí solo continuamos la predicción\n        res += [(mu,P,pred)]\n    \n\n    res = np.array(res)\n    \n    xe = np.array([mu[0] for mu,_,_ in res])\n    xu = np.array([2*np.sqrt(P[0,0]) for _,P,_ in res])\n\n    ye = np.array([mu[1] for mu,_,_ in res])\n    yu = np.array([2*np.sqrt(P[1,1]) for _,P,_ in res])\n    \n    line1.set_data(xe,ye)\n    line2.set_data(xo[1:N],yo[1:N])\n    line3.set_data(xo[max(0,N-1)],yo[max(0,N-1)])  \n    line4.set_data(xe+xu,ye)\n    line5.set_data(xe-xu,ye)\n    \n    return ()\n\nanimation.FuncAnimation(fig, animate, init_func=lambda:[], frames=50, interval=4*1000/25, blit=True)",
      "metadata": {
        "hidden": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}